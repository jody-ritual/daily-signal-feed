<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Signal Feed ‚Äî AI, Web3 & Emerging Trends</title>
    <link rel="stylesheet" href="/daily-signal-feed/css/style.css">
    <meta name="description" content="AI, Web3, and emerging tech news aggregated from 50+ sources with trend detection.">
</head>
<body>
    <header class="site-header">
        <div class="container header-inner">
            <a href="/daily-signal-feed/" class="site-logo">
                <h1>Daily Signal Feed</h1>
                <span class="tagline">AI &bull; Web3 &bull; Emerging Trends</span>
            </a>
            <nav class="main-nav">
                <a href="/daily-signal-feed/" class="active">Home</a>
                
                <a href="/daily-signal-feed/category/ai-llms.html"
                   class="">
                    AI &amp; LLMs (596)
                </a>
                
                <a href="/daily-signal-feed/category/web3-defi.html"
                   class="">
                    Web3 &amp; DeFi (13)
                </a>
                
                <a href="/daily-signal-feed/category/deals.html"
                   class="">
                    Deals &amp; Acquisitions (8)
                </a>
                
                <a href="/daily-signal-feed/category/new-tools.html"
                   class="">
                    New Tools &amp; Apps (9)
                </a>
                
                <a href="/daily-signal-feed/category/regulation.html"
                   class="">
                    Regulation &amp; Policy
                </a>
                
                <a href="/daily-signal-feed/category/social-buzz.html"
                   class="">
                    Social Buzz (60)
                </a>
                
                <a href="/daily-signal-feed/category/emerging-trends.html"
                   class="">
                    Emerging Trends
                </a>
                
                <a href="/daily-signal-feed/archive.html" class="">Archive</a>
            </nav>
        </div>
    </header>

    <main class="container">
        
<!-- Executive Summary -->
<section class="exec-summary">
    <h2>Signal Summary</h2>

    <div class="summary-grid">
        <!-- Trending Topics -->
        <div class="summary-card">
            <h3>Trending Now</h3>
            <ul class="trend-list">
                
                <li class="trend-item">
                    <span class="trend-term">RLVR</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">22 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">Cross</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">9 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">Verifiable Rewards</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">8 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">Test</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">7 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">Retrieval</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">11 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">Yet</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">11 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">SFT</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">13 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
            </ul>
        </div>

        <!-- Category Activity -->
        <div class="summary-card">
            <h3>Category Activity</h3>
            <div class="cat-bars">
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">AI &amp; LLMs</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 100%; background: #4F46E5;"></div>
                    </div>
                    <span class="cat-bar-count">596</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">Social Buzz</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 10%; background: #06B6D4;"></div>
                    </div>
                    <span class="cat-bar-count">60</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">Web3 &amp; DeFi</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 2%; background: #8B5CF6;"></div>
                    </div>
                    <span class="cat-bar-count">13</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">New Tools &amp; Apps</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 2%; background: #F59E0B;"></div>
                    </div>
                    <span class="cat-bar-count">9</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">Deals &amp; Acquisitions</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 1%; background: #EC4899;"></div>
                    </div>
                    <span class="cat-bar-count">8</span>
                </div>
                
                
                
                
                
                
            </div>
        </div>

        <!-- Stats -->
        <div class="summary-card">
            <h3>Today's Signal</h3>
            <div class="stat-grid">
                <div class="stat-item">
                    <div class="stat-value">686</div>
                    <div class="stat-label">Articles</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">19</div>
                    <div class="stat-label">Sources</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">31</div>
                    <div class="stat-label">Trending</div>
                </div>
                <div class="stat-item">
                    <span class="momentum-badge momentum-accelerating">
                        accelerating
                    </span>
                    <div class="stat-label">Momentum</div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Search -->
<input type="text" class="search-bar" placeholder="Search articles by title, source, or keyword..." aria-label="Search articles">

<!-- Trending Articles -->

<section>
    <div class="section-header">
        <h2>Trending Now</h2>
    </div>
    <div class="article-grid">
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/SideProject</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">6m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/SideProject/comments/1r3iwa5/i_built_this_its_simple_here_you_go/" target="_blank" rel="noopener">I built this, it&#39;s simple, here you go.</a>
    </h3>

    
    <p class="card-summary">I got tired of running CLI scripts just to get my codebase into a format ChatGPT could understand. So I built a quick web app (RepoPrint) to do it. What it does: Takes a GitHub URL or a zip file. Generates a single Markdown, PDF, or HTML file. formatting includes the file tree so the LLM knows where files live. Calculates token counts for files. Tech: It&#39;s just React + Vite. All the parsing...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/startups</span>
        <span class="card-category" style="background: #EC4899;">
            Deals &amp; Acquisitions
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/startups/comments/1r3h6ew/my_first_paying_customer_explained_why_our/" target="_blank" rel="noopener">My first paying customer explained why our conversion was terrible- a $1M lesson. I will not promote</a>
    </h3>

    
    <p class="card-summary">I made a call with my first paying customer yesterday. He is not technical, and that one discussion totally transformed the way I view our product. For context: It is an AI assistant that is being developed in Telegram, WhatsApp, Slack, and web chat. Paperwise, the product was powerful. a matter of fact, conversion had a different tale. Stats so far: ~1,500 visitors 2 paying customers 1 refund...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/SideProject</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/SideProject/comments/1r3h4zj/i_built_a_virtual_office_where_8_ai_agents_show/" target="_blank" rel="noopener">I built a virtual office where 8 AI agents show up to work every day.</a>
    </h3>

    
    <p class="card-summary">I built a virtual office where 8 AI agents show up to work every day. Not a chatbot. Not a dashboard. A literal office with desks, a war room, and a lounge where my agents clock in, collaborate, and get things done. Meet the team: - Marco is the CEO. He runs the standups and keeps everyone accountable. - Jimmy crawls content 24/7. He never sleeps, never misses a trend in my niches. - Mona Lisa...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11199" target="_blank" rel="noopener">When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11199v1 Announce Type: new Abstract: Large language models (LLMs) often respond even when prompts omit critical details or include misleading information, leading to hallucinations or reinforced misconceptions. We study how to evaluate and improve LLMs&#39; ability to decide when and what to...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11570" target="_blank" rel="noopener">PRIME: A Process-Outcome Alignment Benchmark for Verifiable Reasoning in Mathematics and Engineering</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11570v1 Announce Type: new Abstract: While model-based verifiers are essential for scaling Reinforcement Learning with Verifiable Rewards (RLVR), current outcome-centric verification paradigms primarily focus on the consistency between the final result and the ground truth, often...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11933" target="_blank" rel="noopener">Cross-Modal Robustness Transfer (CMRT): Training Robust Speech Translation Models Using Adversarial Text</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11933v1 Announce Type: new Abstract: End-to-End Speech Translation (E2E-ST) has seen significant advancements, yet current models are primarily benchmarked on curated, &#34;clean&#34; datasets. This overlooks critical real-world challenges, such as morphological robustness to inflectional...</p>
    

    
</article>
        
    </div>
</section>


<!-- Latest by Date -->

<h3 class="date-label">February 13, 2026</h3>
<div class="article-grid">
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/SideProject</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">6m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/SideProject/comments/1r3iwa5/i_built_this_its_simple_here_you_go/" target="_blank" rel="noopener">I built this, it&#39;s simple, here you go.</a>
    </h3>

    
    <p class="card-summary">I got tired of running CLI scripts just to get my codebase into a format ChatGPT could understand. So I built a quick web app (RepoPrint) to do it. What it does: Takes a GitHub URL or a zip file. Generates a single Markdown, PDF, or HTML file. formatting includes the file tree so the LLM knows where files live. Calculates token counts for files. Tech: It&#39;s just React + Vite. All the parsing...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/SideProject</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">24m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/SideProject/comments/1r3ilkr/you_have_the_audience/" target="_blank" rel="noopener">You have the audience</a>
    </h3>

    
    <p class="card-summary">I noticed a lot of people building sports trackers or fan communities but struggling to bridge the gap between &amp;quot;free users&amp;quot; and &amp;quot;revenue.&amp;quot; ‚ÄãSportsFlux.live handles the monetization heavy lifting. It provides a clean, professional dashboard of live sports (NFL, NBA, NHL) for $3.99/mo, so you don&#39;t have to build the infrastructure yourself. You just drive the traffic, and their...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/OpenAI</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">37m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/OpenAI/comments/1r3idnq/microsoft_ai_chief_confirms_plan_to_ditch_openai/" target="_blank" rel="noopener">Microsoft AI chief confirms plan to ditch OpenAI</a>
    </h3>

    
    <p class="card-summary">Microsoft AI chief confirms plan to ditch OpenAI &amp;#32; submitted by &amp;#32; /u/koffee_addict [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/ChatGPT</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">51m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/ChatGPT/comments/1r3i4ky/_/" target="_blank" rel="noopener">ü§∑üèø‚Äç‚ôÄÔ∏èü§∑üèø‚Äç‚ôÄÔ∏èü§∑üèø‚Äç‚ôÄÔ∏è</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/Responsible-Ship-436 [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/cryptocurrency</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/CryptoCurrency/comments/1r3hsw8/there_are_no_cycles_they_are_coincidences/" target="_blank" rel="noopener">There are no cycles. They are coincidences</a>
    </h3>

    
    <p class="card-summary">The ‚Äúcycle‚Äù is only a series of coincidental events ROUGHLY 3-4 years apart. They have zero to do with halving and are far from predetermined. BTC peaked in 2017. I don‚Äôt know why, but speculative assets were hot. THREE years later In 2020 Covid stimulus and 0% interest rates meant cash was free to borrow and abundant, which led to the big bitcoin run and all the alt coins and NFTs in late 2020...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/ChatGPT</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/ChatGPT/comments/1r3hqsh/chromebooks_need_to_go_away_pencil_and_paper_only/" target="_blank" rel="noopener">Chromebooks need to go away. Pencil and paper only.</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/n8saces [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinTelegraph</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://cointelegraph.com/news/ethzilla-tokenized-jet-engines-rwa-ethereum-liquidity-io?utm_source=rss_feed&amp;utm_medium=rss&amp;utm_campaign=rss_partner_inbound" target="_blank" rel="noopener">ETHZilla offers token tied to jet engine leases amid tokenization pivot</a>
    </h3>

    
    <p class="card-summary">Crypto treasury firm ETHZilla recently sold some of its crypto to begin a tokenization push by buying two jet engines leased to a US airline.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/LocalLLaMA</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/" target="_blank" rel="noopener">UG student launches Dhi-5B (Trained from Scratch)</a>
    </h3>

    
    <p class="card-summary">Hii everyone, I present Dhi-5B: A 5 billion parameter Multimodal Language Model trained compute optimally with just ‚Çπ1.1 lakh ($1200). I incorporate the latest architecture design and training methodologies in this. And I also use a custom built codebase for training these models. I train the Dhi-5B in 5 stages:- üìö Pre-Training: The most compute heavy phase, where the core is built. (Gives the...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/startups</span>
        <span class="card-category" style="background: #EC4899;">
            Deals &amp; Acquisitions
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/startups/comments/1r3hke3/healthtech_femtech_space_startup_i_will_not/" target="_blank" rel="noopener">Healthtech (femtech space) startup. &#34;i will not promote&#34;</a>
    </h3>

    
    <p class="card-summary">I‚Äôm working on Muna Health - a platform run by nurses and powered by AI that supports moms and newborns during the first 42 days after birth, miscarriage, or abortion. It keeps an eye on your health, spots problems early, and helps you get the right care quickly. It also guides you through recovery - physically and emotionally - offers mental health check-ins, and provides advice on sexual and...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/ethereum</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/ethereum/comments/1r3hd2w/daily_general_discussion_february_13_2026/" target="_blank" rel="noopener">Daily General Discussion February 13, 2026</a>
    </h3>

    
    <p class="card-summary">Welcome to the Daily General Discussion on r/ethereum https://imgur.com/3y7vezP Bookmarking this link will always bring you to the current daily: https://old.reddit.com/r/ethereum/about/sticky/?num=2 Please use this thread to discuss Ethereum topics, news, events, and even price! Price discussion posted elsewhere in the subreddit will continue to be removed. As always, be constructive. -...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/startups</span>
        <span class="card-category" style="background: #EC4899;">
            Deals &amp; Acquisitions
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/startups/comments/1r3hcvg/feedback_friday/" target="_blank" rel="noopener">Feedback Friday</a>
    </h3>

    
    <p class="card-summary">Welcome to this week‚Äôs Feedback Thread! Please use this thread appropriately to gather feedback: Feel free to request general feedback or specific feedback in a certain area like user experience, usability, design, landing page(s), or code review You may share surveys You may make an additional request for beta testers Promo codes and affiliates links are ONLY allowed if they are for your product...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/ChatGPT</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/ChatGPT/comments/1r3hats/openai_is_going_to_start_age_verification_selfie/" target="_blank" rel="noopener">OpenAI is going to start Age Verification (selfie or Government ID) according to their Privacy Policy update. Apparently they missed or didn&#39;t care the backlash against Discord doing this a couple days ago.</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/rebbsitor [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/defi</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/defi/comments/1r3halj/what_crypto_journalists_actually_want_to_see_when/" target="_blank" rel="noopener">What crypto journalists actually want to see when evaluating DeFi protocols</a>
    </h3>

    
    <p class="card-summary">Seeing a lot of solid DeFi projects struggle to get coverage in major outlets, so sharing what I&#39;ve learned about what journalists actually prioritize when deciding what to cover. What journalists DON&#39;T want: &amp;quot;Revolutionary&amp;quot; claims without data 10-page technical documentation Generic pitches sent to 50 people Promises of future features What they DO want: Clear problem ‚Üí solution...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/startups</span>
        <span class="card-category" style="background: #EC4899;">
            Deals &amp; Acquisitions
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/startups/comments/1r3h6ew/my_first_paying_customer_explained_why_our/" target="_blank" rel="noopener">My first paying customer explained why our conversion was terrible- a $1M lesson. I will not promote</a>
    </h3>

    
    <p class="card-summary">I made a call with my first paying customer yesterday. He is not technical, and that one discussion totally transformed the way I view our product. For context: It is an AI assistant that is being developed in Telegram, WhatsApp, Slack, and web chat. Paperwise, the product was powerful. a matter of fact, conversion had a different tale. Stats so far: ~1,500 visitors 2 paying customers 1 refund...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/SideProject</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/SideProject/comments/1r3h4zj/i_built_a_virtual_office_where_8_ai_agents_show/" target="_blank" rel="noopener">I built a virtual office where 8 AI agents show up to work every day.</a>
    </h3>

    
    <p class="card-summary">I built a virtual office where 8 AI agents show up to work every day. Not a chatbot. Not a dashboard. A literal office with desks, a war room, and a lounge where my agents clock in, collaborate, and get things done. Meet the team: - Marco is the CEO. He runs the standups and keeps everyone accountable. - Jimmy crawls content 24/7. He never sleeps, never misses a trend in my niches. - Mona Lisa...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/StableDiffusion</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/StableDiffusion/comments/1r3h2xy/whats_the_best_recommended_video_upscaler_for/" target="_blank" rel="noopener">What&#39;s the best recommended video upscaler for 16GB VRAM?</a>
    </h3>

    
    <p class="card-summary">This is the only video upscaler I&#39;ve tried: https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler I want to upscale 20-30 second long 360p videos (500-750 frames), but my main issue with it is that upscaling to 720p takes 15+ minutes on my 5070 Ti. I can try upscaling to 540p and it only takes 8 minutes, but that&#39;s still a lot longer than I&#39;d prefer. Upscaling to 480p only takes 5 minutes, but...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/cryptocurrency</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/CryptoCurrency/comments/1r3gtv0/el_salvadors_bitcoin_conviction_now_carries_a_300/" target="_blank" rel="noopener">El Salvador‚Äôs Bitcoin Conviction Now Carries a $300 Million Price Tag</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/DirectionMundane5468 [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/cryptocurrency</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/CryptoCurrency/comments/1r3gtg5/zerebro_founder_jeffy_yu_has_allegedly_killed/" target="_blank" rel="noopener">Zerebro founder Jeffy Yu has allegedly killed himself again</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/DirectionMundane5468 [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/MachineLearning</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/MachineLearning/comments/1r3gqng/r_has_anyone_experimented_with_mhc_on_traditional/" target="_blank" rel="noopener">[R] Has anyone experimented with MHC on traditional autoencoders/convolutional architectures?</a>
    </h3>

    
    <p class="card-summary">I&#39;m currently making a baseline autoencoder for this super freaking huge hyperspectral image dataset I have. It&#39;s a really big pain to work with and to get decent results, and I had to basically pull all stops including using ResNeXt2, channel-by-channel processing and grouping, etc. I&#39;m considering replacing all the residual connections with MHc. But I don&#39;t have any experience with it, so I...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/LocalLLaMA</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/LocalLLaMA/comments/1r3gqlf/i_have_access_to_airawat_supercomputer_a100h100/" target="_blank" rel="noopener">I have access to AIRAWAT Supercomputer (A100/H100 cluster). Looking for a team to fine-tune Wan 2.1-14B for Education.</a>
    </h3>

    
    <p class="card-summary">Hi everyone, I&#39;m building an education engine (Zulense) to visualize Indian curriculums (NCERT). I see everyone struggling to run Wan 2.1 on consumer cards, but I have the opposite problem: I have the compute (AIRAWAT), but I need the engineering talent. I want to run a full parameter fine-tune (not just LoRA) on a massive video dataset of blackboard teaching. If you&#39;ve been wanting to test a...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinTelegraph</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://cointelegraph.com/news/crypto-use-in-human-trafficking-networks-surged-in-2025-chainalysis?utm_source=rss_feed&amp;utm_medium=rss&amp;utm_campaign=rss_partner_inbound" target="_blank" rel="noopener">Crypto use in human trafficking networks surged in 2025: Chainalysis</a>
    </h3>

    
    <p class="card-summary">Chainalysis says that despite the rising use of crypto in trafficking networks, the transparency of blockchain may give visibility into the operations, aiding law enforcement.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/OpenAI</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/OpenAI/comments/1r3gm35/api_keys_confuse_me_please_explain_how_i_can_make/" target="_blank" rel="noopener">API keys confuse me , please explain how I can make a compatible one that works?</a>
    </h3>

    
    <p class="card-summary">I apologize if this is a stupid question, hopefully it‚Äôs allowed. I want to chat with janitor ai using an API key. I have tried openai and OpenRouter. I create one, plug it into janitor, and it continually gives me different errors. Error processing your request and no response from bot. I have credits on my open router account.. thanks &amp;#32; submitted by &amp;#32; /u/Interesting_Shape_13 [link]...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/LocalLLaMA</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/LocalLLaMA/comments/1r3gjx5/zwz_8b7b4b/" target="_blank" rel="noopener">ZwZ 8B/7B/4B</a>
    </h3>

    
    <p class="card-summary">Model Summary ZwZ-8B is a fine-grained multimodal perception model built upon Qwen3-VL-8B. It is trained using Region-to-Image Distillation (R2I) combined with reinforcement learning, enabling superior fine-grained visual understanding in a single forward pass ‚Äî no inference-time zooming or tool calling required. ZwZ-8B achieves state-of-the-art performance on fine-grained perception benchmarks...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinTelegraph</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://cointelegraph.com/news/israel-arrests-two-polymarket-trades-militiary-strikes?utm_source=rss_feed&amp;utm_medium=rss&amp;utm_campaign=rss_partner_inbound" target="_blank" rel="noopener">Israel arrests two over Polymarket trades on military operations</a>
    </h3>

    
    <p class="card-summary">Israeli authorities said a military reservist and a civilian were arrested after allegedly using classified information to place bets related to military strikes on Iran.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">Hacker News - Front Page</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://nickbostrom.com/optimal.pdf" target="_blank" rel="noopener">New Nick Bostrom Paper: Optimal Timing for Superintelligence [pdf]</a>
    </h3>

    
    <p class="card-summary">Article URL: https://nickbostrom.com/optimal.pdf Comments URL: https://news.ycombinator.com/item?id=46999117 Points: 36 # Comments: 26</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/artificial</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/artificial/comments/1r3gbrx/1password_open_sources_a_benchmark_to_stop_ai/" target="_blank" rel="noopener">1Password open sources a benchmark to stop AI agents from leaking credentials</a>
    </h3>

    
    <p class="card-summary">The benchmark tests whether AI agents behave safely during real workflows, including opening emails, clicking links, retrieving stored credentials, and filling out login forms. &amp;#32; submitted by &amp;#32; /u/tekz [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/cryptocurrency</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/CryptoCurrency/comments/1r3ga22/a_strategic_shift_european_union_evaluates/" target="_blank" rel="noopener">A Strategic Shift: European Union Evaluates Ethereum as Infrastructure for the Digital Euro</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/x___rain [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11156" target="_blank" rel="noopener">HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&amp;amp;A over Raw Unstructured Documents</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11156v1 Announce Type: new Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11157" target="_blank" rel="noopener">Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11157v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly deployed worldwide, yet their safety alignment remains predominantly English-centric. This allows for vulnerabilities in non-English contexts, especially with low-resource languages. We introduce a novel...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11162" target="_blank" rel="noopener">Retrieval Heads are Dynamic</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11162v1 Announce Type: new Abstract: Recent studies have identified &#34;retrieval heads&#34; in Large Language Models (LLMs) responsible for extracting information from input contexts. However, prior works largely rely on static statistics aggregated across datasets, identifying heads that...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11163" target="_blank" rel="noopener">Nested Named Entity Recognition in Plasma Physics Research Articles</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11163v1 Announce Type: new Abstract: Named Entity Recognition (NER) is an important task in natural language processing that aims to identify and extract key entities from unstructured text. We present a novel application of NER in plasma physics research articles and address the...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11165" target="_blank" rel="noopener">Assessing LLM Reliability on Temporally Recent Open-Domain Questions</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11165v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly deployed for open-domain question answering, yet their alignment with human perspectives on temporally recent information remains underexplored. We introduce RECOM (Reddit Evaluation for Correspondence of...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11166" target="_blank" rel="noopener">Small Updates, Big Doubts: Does Parameter-Efficient Fine-tuning Enhance Hallucination Detection ?</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11166v1 Announce Type: new Abstract: Parameter-efficient fine-tuning (PEFT) methods are widely used to adapt large language models (LLMs) to downstream tasks and are often assumed to improve factual correctness. However, how the parameter-efficient fine-tuning methods affect...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11167" target="_blank" rel="noopener">Visualizing and Benchmarking LLM Factual Hallucination Tendencies via Internal State Analysis and Clustering</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11167v1 Announce Type: new Abstract: Large Language Models (LLMs) often hallucinate, generating nonsensical or false information that can be especially harmful in sensitive fields such as medicine or law. To study this phenomenon systematically, we introduce FalseCite, a curated dataset...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11168" target="_blank" rel="noopener">Enhancing SDG-Text Classification with Combinatorial Fusion Analysis and Generative AI</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11168v1 Announce Type: new Abstract: (Natural Language Processing) NLP techniques such as text classification and topic discovery are very useful in many application areas including information retrieval, knowledge discovery, policy formulation, and decision-making. However, it remains a...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11169" target="_blank" rel="noopener">Disentangling Direction and Magnitude in Transformer Representations: A Double Dissociation Through L2-Matched Perturbation Analysis</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11169v1 Announce Type: new Abstract: Transformer hidden states encode information as high-dimensional vectors, yet whether direction (orientation in representational space) and magnitude (vector norm) serve distinct functional roles remains unclear. Studying Pythia-family models, we...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11170" target="_blank" rel="noopener">PRIME: Policy-Reinforced Iterative Multi-agent Execution for Algorithmic Reasoning in Large Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11170v1 Announce Type: new Abstract: Large language models have demonstrated remarkable capabilities across diverse reasoning tasks, yet their performance on algorithmic reasoning remains limited. To handle this limitation, we propose PRIME (Policy-Reinforced Iterative Multi-agent...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11171" target="_blank" rel="noopener">Efficient Hyper-Parameter Search for LoRA via Language-aided Bayesian Optimization</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11171v1 Announce Type: new Abstract: Fine-tuning Large Language Models (LLMs) with Low-Rank Adaptation (LoRA) enables resource-efficient personalization or specialization, but it comes at the expense of additional hyperparameter tuning. Although LoRA makes fine-tuning efficient, it is...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11172" target="_blank" rel="noopener">Synthesizing the Virtual Advocate: A Multi-Persona Speech Generation Framework for Diverse Linguistic Jurisdictions in Indic Languages</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11172v1 Announce Type: new Abstract: Legal advocacy requires a unique combination of authoritative tone, rhythmic pausing for emphasis, and emotional intelligence. This study investigates the performance of the Gemini 2.5 Flash TTS and Gemini 2.5 Pro TTS models in generating synthetic...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11173" target="_blank" rel="noopener">Author-in-the-Loop Response Generation and Evaluation: Integrating Author Expertise and Intent in Responses to Peer Review</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11173v1 Announce Type: new Abstract: Author response (rebuttal) writing is a critical stage of scientific peer review that demands substantial author effort. Recent work frames this task as automatic text generation, underusing author expertise and intent. In practice, authors possess...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11174" target="_blank" rel="noopener">The Script Tax: Measuring Tokenization-Driven Efficiency and Latency Disparities in Multilingual Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11174v1 Announce Type: new Abstract: Pretrained multilingual language models are often assumed to be script-agnostic, yet their tokenizers can impose systematic costs on certain writing systems. We quantify this script tax by comparing two orthographic variants with identical linguistic...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11175" target="_blank" rel="noopener">Barriers to Discrete Reasoning with Transformers: A Survey Across Depth, Exactness, and Bandwidth</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11175v1 Announce Type: new Abstract: Transformers have become the foundational architecture for a broad spectrum of sequence modeling applications, underpinning state-of-the-art systems in natural language processing, vision, and beyond. However, their theoretical limitations in discrete...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11176" target="_blank" rel="noopener">Evaluating Few-Shot Temporal Reasoning of LLMs for Human Activity Prediction in Smart Environments</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11176v1 Announce Type: new Abstract: Anticipating human activities and their durations is essential in applications such as smart-home automation, simulation-based architectural and urban design, activity-based transportation system simulation, and human-robot collaboration, where...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11177" target="_blank" rel="noopener">What Do LLMs Know About Alzheimer&#39;s Disease? Fine-Tuning, Probing, and Data Synthesis for AD Detection</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11177v1 Announce Type: new Abstract: Reliable early detection of Alzheimer&#39;s disease (AD) is challenging, particularly due to limited availability of labeled data. While large language models (LLMs) have shown strong transfer capabilities across domains, adapting them to the AD domain...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11179" target="_blank" rel="noopener">From Instruction to Output: The Role of Prompting in Modern NLG</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11179v1 Announce Type: new Abstract: Prompt engineering has emerged as an integral technique for extending the strengths and abilities of Large Language Models (LLMs) to gain significant performance gains in various Natural Language Processing (NLP) tasks. This approach, which requires...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11180" target="_blank" rel="noopener">Mechanistic Interpretability for Large Language Model Alignment: Progress, Challenges, and Future Directions</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11180v1 Announce Type: new Abstract: Large language models (LLMs) have achieved remarkable capabilities across diverse tasks, yet their internal decision-making processes remain largely opaque. Mechanistic interpretability (i.e., the systematic study of how neural networks implement...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11181" target="_blank" rel="noopener">Code Mixologist : A Practitioner&#39;s Guide to Building Code-Mixed LLMs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11181v1 Announce Type: new Abstract: Code-mixing and code-switching (CSW) remain challenging phenomena for large language models (LLMs). Despite recent advances in multilingual modeling, LLMs often struggle in mixed-language settings, exhibiting systematic degradation in grammaticality...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11182" target="_blank" rel="noopener">MetaMem: Evolving Meta-Memory for Knowledge Utilization through Self-Reflective Symbolic Optimization</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11182v1 Announce Type: new Abstract: Existing memory systems enable Large Language Models (LLMs) to support long-horizon human-LLM interactions by persisting historical interactions beyond limited context windows. However, while recent approaches have succeeded in constructing effective...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11198" target="_blank" rel="noopener">DDL2PropBank Agent: Benchmarking Multi-Agent Frameworks&#39; Developer Experience Through a Novel Relational Schema Mapping Task</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11198v1 Announce Type: new Abstract: Multi-agent frameworks promise to simplify LLM-driven software development, yet there is no principled way to evaluate their developer experience in a controlled setting. We introduce DDL2PropBank, a novel benchmark task that maps relational database...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11199" target="_blank" rel="noopener">When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11199v1 Announce Type: new Abstract: Large language models (LLMs) often respond even when prompts omit critical details or include misleading information, leading to hallucinations or reinforced misconceptions. We study how to evaluate and improve LLMs&#39; ability to decide when and what to...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11201" target="_blank" rel="noopener">Mechanistic Evidence for Faithfulness Decay in Chain-of-Thought Reasoning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11201v1 Announce Type: new Abstract: Chain-of-Thought (CoT) explanations are widely used to interpret how language models solve complex problems, yet it remains unclear whether these step-by-step explanations reflect how the model actually reaches its answer, or merely post-hoc...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11221" target="_blank" rel="noopener">The Automatic Verification of Image-Text Claims (AVerImaTeC) Shared Task</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11221v1 Announce Type: new Abstract: The Automatic Verification of Image-Text Claims (AVerImaTeC) shared task aims to advance system development for retrieving evidence and verifying real-world image-text claims. Participants were allowed to either employ external knowledge sources, such...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11238" target="_blank" rel="noopener">SurveyLens: A Research Discipline-Aware Benchmark for Automatic Survey Generation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11238v1 Announce Type: new Abstract: The exponential growth of scientific literature has driven the evolution of Automatic Survey Generation (ASG) from simple pipelines to multi-agent frameworks and commercial Deep Research agents. However, current ASG evaluation methods rely on generic...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11305" target="_blank" rel="noopener">Are Aligned Large Language Models Still Misaligned?</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11305v1 Announce Type: new Abstract: Misalignment in Large Language Models (LLMs) arises when model behavior diverges from human expectations and fails to simultaneously satisfy safety, value, and cultural dimensions, which must co-occur in real-world settings to solve a real-world...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11328" target="_blank" rel="noopener">Evaluating Alignment of Behavioral Dispositions in LLMs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11328v1 Announce Type: new Abstract: As LLMs integrate into our daily lives, understanding their behavior becomes essential. In this work, we focus on behavioral dispositions$-$the underlying tendencies that shape responses in social contexts$-$and introduce a framework to study how...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11358" target="_blank" rel="noopener">When Models Examine Themselves: Vocabulary-Activation Correspondence in Self-Referential Processing</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11358v1 Announce Type: new Abstract: Large language models produce rich introspective language when prompted for self-examination, but whether this language reflects internal computation or sophisticated confabulation has remained unclear. We show that self-referential vocabulary tracks...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11361" target="_blank" rel="noopener">Finding the Cracks: Improving LLMs Reasoning with Paraphrastic Probing and Consistency Verification</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11361v1 Announce Type: new Abstract: Large language models have demonstrated impressive performance across a variety of reasoning tasks. However, their problem-solving ability often declines on more complex tasks due to hallucinations and the accumulation of errors within these...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11364" target="_blank" rel="noopener">The Energy of Falsehood: Detecting Hallucinations via Diffusion Model Likelihoods</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11364v1 Announce Type: new Abstract: Large Language Models (LLMs) frequently hallucinate plausible but incorrect assertions, a vulnerability often missed by uncertainty metrics when models are confidently wrong. We propose DiffuTruth, an unsupervised framework that reconceptualizes fact...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11391" target="_blank" rel="noopener">Advancing AI Trustworthiness Through Patient Simulation: Risk Assessment of Conversational Agents for Antidepressant Selection</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11391v1 Announce Type: new Abstract: Objective: This paper introduces a patient simulator designed to enable scalable, automated evaluation of healthcare conversational agents. The simulator generates realistic, controllable patient interactions that systematically vary across medical...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11424" target="_blank" rel="noopener">Gradients Must Earn Their Influence: Unifying SFT with Generalized Entropic Objectives</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11424v1 Announce Type: new Abstract: Standard negative log-likelihood (NLL) for Supervised Fine-Tuning (SFT) applies uniform token-level weighting. This rigidity creates a two-fold failure mode: (i) overemphasizing low-probability targets can amplify gradients on noisy supervision and...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11444" target="_blank" rel="noopener">Towards Reliable Machine Translation: Scaling LLMs for Critical Error Detection and Safety</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11444v1 Announce Type: new Abstract: Machine Translation (MT) plays a pivotal role in cross-lingual information access, public policy communication, and equitable knowledge dissemination. However, critical meaning errors, such as factual distortions, intent reversals, or biased...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11451" target="_blank" rel="noopener">LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11451v1 Announce Type: new Abstract: Looped Transformers have emerged as an efficient and powerful class of models for reasoning in the language domain. Recent studies show that these models achieve strong performance on algorithmic and reasoning tasks, suggesting that looped...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11460" target="_blank" rel="noopener">ADRD-Bench: A Preliminary LLM Benchmark for Alzheimer&#39;s Disease and Related Dementias</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11460v1 Announce Type: new Abstract: Large language models (LLMs) have shown great potential for healthcare applications. However, existing evaluation benchmarks provide minimal coverage of Alzheimer&#39;s Disease and Related Dementias (ADRD). To address this gap, we introduce ADRD-Bench...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11488" target="_blank" rel="noopener">When Audio-LLMs Don&#39;t Listen: A Cross-Linguistic Study of Modality Arbitration</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11488v1 Announce Type: new Abstract: When audio and text conflict, speech-enabled language models follow the text 10 times more often than when arbitrating between two text sources, even when explicitly instructed to trust the audio. Using ALME, a benchmark of 57,602 controlled...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11509" target="_blank" rel="noopener">Multimodal Fact-Level Attribution for Verifiable Reasoning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11509v1 Announce Type: new Abstract: Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11543" target="_blank" rel="noopener">Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11543v1 Announce Type: new Abstract: Pretraining large language models (LLMs) typically requires centralized clusters with thousands of high-memory GPUs (e.g., H100/A100). Recent decentralized training methods reduce communication overhead by employing federated optimization; however...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11551" target="_blank" rel="noopener">SIGHT: Reinforcement Learning with Self-Evidence and Information-Gain Diverse Branching for Search Agent</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11551v1 Announce Type: new Abstract: Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to master autonomous search for complex question answering. However, particularly within multi-turn search scenarios, this interaction introduces a critical challenge: search...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11570" target="_blank" rel="noopener">PRIME: A Process-Outcome Alignment Benchmark for Verifiable Reasoning in Mathematics and Engineering</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11570v1 Announce Type: new Abstract: While model-based verifiers are essential for scaling Reinforcement Learning with Verifiable Rewards (RLVR), current outcome-centric verification paradigms primarily focus on the consistency between the final result and the ground truth, often...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11607" target="_blank" rel="noopener">Scene-Aware Memory Discrimination: Deciding Which Personal Knowledge Stays</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11607v1 Announce Type: new Abstract: Intelligent devices have become deeply integrated into everyday life, generating vast amounts of user interactions that form valuable personal knowledge. Efficient organization of this knowledge in user memory is essential for enabling personalized...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11639" target="_blank" rel="noopener">PACE: Prefix-Protected and Difficulty-Aware Compression for Efficient Reasoning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11639v1 Announce Type: new Abstract: Language Reasoning Models (LRMs) achieve strong performance by scaling test-time computation but often suffer from ``overthinking&#39;&#39;, producing excessively long reasoning traces that increase latency and memory usage. Existing LRMs typically enforce...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11650" target="_blank" rel="noopener">Which Feedback Works for Whom? Differential Effects of LLM-Generated Feedback Elements Across Learner Profiles</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11650v1 Announce Type: new Abstract: Large language models (LLMs) show promise for automatically generating feedback in education settings. However, it remains unclear how specific feedback elements, such as tone and information coverage, contribute to learning outcomes and learner...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11684" target="_blank" rel="noopener">PatientHub: A Unified Framework for Patient Simulation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11684v1 Announce Type: new Abstract: As Large Language Models increasingly power role-playing applications, simulating patients has become a valuable tool for training counselors and scaling therapeutic assessment. However, prior work is fragmented: existing approaches rely on...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11699" target="_blank" rel="noopener">Finding Sense in Nonsense with Generated Contexts: Perspectives from Humans and Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11699v1 Announce Type: new Abstract: Nonsensical and anomalous sentences have been instrumental in the development of computational models of semantic interpretation. A core challenge is to distinguish between what is merely anomalous (but can be interpreted given a supporting context)...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11731" target="_blank" rel="noopener">Thinking with Drafting: Optical Decompression via Logical Reconstruction</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11731v1 Announce Type: new Abstract: Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11748" target="_blank" rel="noopener">Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11748v1 Announce Type: new Abstract: Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context. Grounded in State Coverage theory, our...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11761" target="_blank" rel="noopener">MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11761v1 Announce Type: new Abstract: The evolution of large language models (LLMs) towards applications with ultra-long contexts faces challenges posed by the high computational and memory costs of the Transformer architecture. While existing sparse and linear attention mechanisms...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11795" target="_blank" rel="noopener">A Subword Embedding Approach for Variation Detection in Luxembourgish User Comments</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11795v1 Announce Type: new Abstract: This paper presents an embedding-based approach to detecting variation without relying on prior normalisation or predefined variant lists. The method trains subword embeddings on raw text and groups related forms through combined cosine and n-gram...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11871" target="_blank" rel="noopener">DMAP: A Distribution Map for Text</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11871v1 Announce Type: new Abstract: Large Language Models (LLMs) are a powerful tool for statistical text analysis, with derived sequences of next-token probability distributions offering a wealth of information. Extracting this signal typically relies on metrics such as perplexity...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11877" target="_blank" rel="noopener">Towards Fair and Comprehensive Evaluation of Routers in Collaborative LLM Systems</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11877v1 Announce Type: new Abstract: Large language models (LLMs) have achieved success, but cost and privacy constraints necessitate deploying smaller models locally while offloading complex queries to cloud-based models. Existing router evaluations are unsystematic, overlooking...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11886" target="_blank" rel="noopener">LLM-based Triplet Extraction from Financial Reports</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11886v1 Announce Type: new Abstract: Corporate financial reports are a valuable source of structured knowledge for Knowledge Graph construction, but the lack of annotated ground truth in this domain makes evaluation difficult. We present a semi-automated pipeline for...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11898" target="_blank" rel="noopener">Benchmark Illusion: Disagreement among LLMs and Its Scientific Consequences</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11898v1 Announce Type: new Abstract: Benchmarks underpin how progress in large language models (LLMs) is measured and trusted. Yet our analyses reveal that apparent convergence in benchmark accuracy can conceal deep epistemic divergence. Using two major reasoning benchmarks - MMLU-Pro...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11931" target="_blank" rel="noopener">AdaptEvolve: Improving Efficiency of Evolutionary AI Agents through Adaptive Model Selection</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11931v1 Announce Type: new Abstract: Evolutionary agentic systems intensify the trade-off between computational efficiency and reasoning capability by repeatedly invoking large language models (LLMs) during inference. This setting raises a central question: how can an agent dynamically...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11933" target="_blank" rel="noopener">Cross-Modal Robustness Transfer (CMRT): Training Robust Speech Translation Models Using Adversarial Text</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11933v1 Announce Type: new Abstract: End-to-End Speech Translation (E2E-ST) has seen significant advancements, yet current models are primarily benchmarked on curated, &#34;clean&#34; datasets. This overlooks critical real-world challenges, such as morphological robustness to inflectional...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11938" target="_blank" rel="noopener">Who is the richest club in the championship? Detecting and Rewriting Underspecified Questions Improve QA Performance</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11938v1 Announce Type: new Abstract: Large language models (LLMs) perform well on well-posed questions, yet standard question-answering (QA) benchmarks remain far from solved. We argue that this gap is partly due to underspecified questions - queries whose interpretation cannot be...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11939" target="_blank" rel="noopener">Do Large Language Models Adapt to Language Variation across Socioeconomic Status?</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11939v1 Announce Type: new Abstract: Humans adjust their linguistic style to the audience they are addressing. However, the extent to which LLMs adapt to different social contexts is largely unknown. As these models increasingly mediate human-to-human communication, their failure to...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11961" target="_blank" rel="noopener">Scaling Model and Data for Multilingual Machine Translation with Open Large Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11961v1 Announce Type: new Abstract: Open large language models (LLMs) have demonstrated improving multilingual capabilities in recent years. In this paper, we present a study of open LLMs for multilingual machine translation (MT) across a range of languages, and investigate the effects...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11968" target="_blank" rel="noopener">DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11968v1 Announce Type: new Abstract: In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11982" target="_blank" rel="noopener">Automatic Simplification of Common Vulnerabilities and Exposures Descriptions</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11982v1 Announce Type: new Abstract: Understanding cyber security is increasingly important for individuals and organizations. However, a lot of information related to cyber security can be difficult to understand to those not familiar with the topic. In this study, we focus on...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12005" target="_blank" rel="noopener">LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12005v1 Announce Type: new Abstract: Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12015" target="_blank" rel="noopener">Disentangling Ambiguity from Instability in Large Language Models: A Clinical Text-to-SQL Case Study</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12015v1 Announce Type: new Abstract: Deploying large language models for clinical Text-to-SQL requires distinguishing two qualitatively different causes of output diversity: (i) input ambiguity that should trigger clarification, and (ii) model instability that should trigger human...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12036" target="_blank" rel="noopener">Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12036v1 Announce Type: new Abstract: Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12092" target="_blank" rel="noopener">DeepSight: An All-in-One LM Safety Toolkit</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12092v1 Announce Type: new Abstract: As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12116" target="_blank" rel="noopener">P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12116v1 Announce Type: new Abstract: Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12132" target="_blank" rel="noopener">A Rule-based Computational Model for Gaidhlig Morphology</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12132v1 Announce Type: new Abstract: Language models and software tools are essential to support the continuing vitality of lesser-used languages; however, currently popular neural models require considerable data for training, which normally is not available for such low-resource...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12135" target="_blank" rel="noopener">WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12135v1 Announce Type: new Abstract: With the rapid integration of advanced reasoning capabilities into spoken dialogue models, the field urgently demands benchmarks that transcend simple interactions to address real-world complexity. However, current evaluations predominantly adhere to...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12137" target="_blank" rel="noopener">CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12137v1 Announce Type: new Abstract: City councils play a crucial role in local governance, directly influencing citizens&#39; daily lives through decisions made during municipal meetings. These deliberations are formally documented in meeting minutes, which serve as official records of...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12153" target="_blank" rel="noopener">dVoting: Fast Voting for dLLMs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12153v1 Announce Type: new Abstract: Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12192" target="_blank" rel="noopener">Query-focused and Memory-aware Reranker for Long Context Processing</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12192v1 Announce Type: new Abstract: Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12196" target="_blank" rel="noopener">Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12196v1 Announce Type: new Abstract: AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12203" target="_blank" rel="noopener">ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12203v1 Announce Type: new Abstract: Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12235" target="_blank" rel="noopener">Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12235v1 Announce Type: new Abstract: Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12241" target="_blank" rel="noopener">Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12241v1 Announce Type: new Abstract: Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12251" target="_blank" rel="noopener">A technical curriculum on language-oriented artificial intelligence in translation and specialised communication</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12251v1 Announce Type: new Abstract: This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&amp;amp;T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12262" target="_blank" rel="noopener">T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12262v1 Announce Type: new Abstract: Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.12275" target="_blank" rel="noopener">On-Policy Context Distillation for Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.12275v1 Announce Type: new Abstract: Context distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11161" target="_blank" rel="noopener">Althea: Human-AI Collaboration for Fact-Checking and Critical Reasoning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11161v1 Announce Type: cross Abstract: The web&#39;s information ecosystem demands fact-checking systems that are both scalable and epistemically trustworthy. Automated approaches offer efficiency but often lack transparency, while human verification remains slow and inconsistent. We...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11164" target="_blank" rel="noopener">Automated Optimization Modeling via a Localizable Error-Driven Perspective</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11164v1 Announce Type: cross Abstract: Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs&#39; capabilities in this domain, its...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11220" target="_blank" rel="noopener">Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11220v1 Announce Type: cross Abstract: Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model&#39;s prior training...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11224" target="_blank" rel="noopener">Agent-Diff: Benchmarking LLM Agents on Enterprise API Tasks via Code Execution with State-Diff-Based Evaluation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11224v1 Announce Type: cross Abstract: We present Agent-Diff, a novel benchmarking framework for evaluating agentic Large Language Models (LLMs) on real-world tasks that execute code via external APIs. Agentic LLM performance varies due to differences in models, external tool access...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11236" target="_blank" rel="noopener">ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11236v1 Announce Type: cross Abstract: Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the &#39;&#39;one-brain, many-forms&#39;&#39; paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11243" target="_blank" rel="noopener">Evaluating Memory Structure in LLM Agents</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11243v1 Announce Type: cross Abstract: Modern LLM-based agents and chat assistants rely on long-term memory frameworks to store reusable knowledge, recall user preferences, and augment reasoning. As researchers create more complex memory architectures, it becomes increasingly difficult...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11246" target="_blank" rel="noopener">How Many Features Can a Language Model Store Under the Linear Representation Hypothesis?</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11246v1 Announce Type: cross Abstract: We introduce a mathematical framework for the linear representation hypothesis (LRH), which asserts that intermediate layers of language models store features linearly. We separate the hypothesis into two claims: linear representation (features are...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11318" target="_blank" rel="noopener">Dissecting Subjectivity and the &#34;Ground Truth&#34; Illusion in Data Annotation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11318v1 Announce Type: cross Abstract: In machine learning, &#34;ground truth&#34; refers to the assumed correct labels used to train and evaluate models. However, the foundational &#34;ground truth&#34; paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11354" target="_blank" rel="noopener">ReplicatorBench: Benchmarking LLM Agents for Replicability in Social and Behavioral Sciences</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11354v1 Announce Type: cross Abstract: The literature has witnessed an emerging interest in AI agents for automated assessment of scientific papers. Existing benchmarks focus primarily on the computational aspect of this task, testing agents&#39; ability to reproduce or replicate research...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11388" target="_blank" rel="noopener">Sparse Semantic Dimension as a Generalization Certificate for LLMs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11388v1 Announce Type: cross Abstract: Standard statistical learning theory predicts that Large Language Models (LLMs) should overfit because their parameter counts vastly exceed the number of training tokens. Yet, in practice, they generalize robustly. We propose that the effective...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11495" target="_blank" rel="noopener">Jailbreaking Leaves a Trace: Understanding and Detecting Jailbreak Attacks from Internal Representations of Large Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11495v1 Announce Type: cross Abstract: Jailbreaking large language models (LLMs) has emerged as a critical security challenge with the widespread deployment of conversational AI systems. Adversarial users exploit these models through carefully crafted prompts to elicit restricted or...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11524" target="_blank" rel="noopener">Adaptive Milestone Reward for GUI Agents</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11524v1 Announce Type: cross Abstract: Reinforcement Learning (RL) has emerged as a mainstream paradigm for training Mobile GUI Agents, yet it struggles with the temporal credit assignment problem inherent in long-horizon tasks. A primary challenge lies in the trade-off between reward...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11528" target="_blank" rel="noopener">Stop Tracking Me! Proactive Defense Against Attribute Inference Attack in LLMs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11528v1 Announce Type: cross Abstract: Recent studies have shown that large language models (LLMs) can infer private user attributes (e.g., age, location, gender) from user-generated text shared online, enabling rapid and large-scale privacy breaches. Existing anonymization-based...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11581" target="_blank" rel="noopener">Analytical Search</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11581v1 Announce Type: cross Abstract: Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11666" target="_blank" rel="noopener">PhyNiKCE: A Neurosymbolic Agentic Framework for Autonomous Computational Fluid Dynamics</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11666v1 Announce Type: cross Abstract: The deployment of autonomous agents for Computational Fluid Dynamics (CFD), is critically limited by the probabilistic nature of Large Language Models (LLMs), which struggle to enforce the strict conservation laws and numerical stability required...</p>
    

    
</article>
    
</div>


    </main>

    <footer class="site-footer">
        <div class="container">
            <p>Last updated: February 13, 2026 at 07:35 UTC</p>
            <p>686 articles from 19 sources</p>
            <p>Daily Signal Feed &mdash; AI, Web3 &amp; Emerging Tech Aggregator</p>
        </div>
    </footer>

    <script>
    // Client-side search filter
    document.addEventListener('DOMContentLoaded', function() {
        const searchBar = document.querySelector('.search-bar');
        if (searchBar) {
            searchBar.addEventListener('input', function(e) {
                const query = e.target.value.toLowerCase();
                const cards = document.querySelectorAll('.article-card, .archive-item');
                cards.forEach(function(card) {
                    const text = card.textContent.toLowerCase();
                    card.style.display = text.includes(query) ? '' : 'none';
                });
            });
        }
    });
    </script>
</body>
</html>