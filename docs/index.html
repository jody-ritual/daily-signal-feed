<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Signal Feed — AI, Web3 & Emerging Trends</title>
    <link rel="stylesheet" href="/daily-signal-feed/css/style.css">
    <meta name="description" content="AI, Web3, and emerging tech news aggregated from 50+ sources with trend detection.">
</head>
<body>
    <header class="site-header">
        <div class="container header-inner">
            <a href="/daily-signal-feed/" class="site-logo">
                <h1>Daily Signal Feed</h1>
                <span class="tagline">AI &bull; Web3 &bull; Emerging Trends</span>
            </a>
            <nav class="main-nav">
                <a href="/daily-signal-feed/" class="active">Home</a>
                
                <a href="/daily-signal-feed/category/ai-llms.html"
                   class="">
                    AI &amp; LLMs (904)
                </a>
                
                <a href="/daily-signal-feed/category/web3-defi.html"
                   class="">
                    Web3 &amp; DeFi (15)
                </a>
                
                <a href="/daily-signal-feed/category/deals.html"
                   class="">
                    Deals &amp; Acquisitions (3)
                </a>
                
                <a href="/daily-signal-feed/category/new-tools.html"
                   class="">
                    New Tools &amp; Apps (7)
                </a>
                
                <a href="/daily-signal-feed/category/regulation.html"
                   class="">
                    Regulation &amp; Policy
                </a>
                
                <a href="/daily-signal-feed/category/social-buzz.html"
                   class="">
                    Social Buzz (61)
                </a>
                
                <a href="/daily-signal-feed/category/emerging-trends.html"
                   class="">
                    Emerging Trends
                </a>
                
                <a href="/daily-signal-feed/archive.html" class="">Archive</a>
            </nav>
        </div>
    </header>

    <main class="container">
        
<!-- Executive Summary -->
<section class="exec-summary">
    <h2>Signal Summary</h2>

    <div class="summary-grid">
        <!-- Trending Topics -->
        <div class="summary-card">
            <h3>Trending Now</h3>
            <ul class="trend-list">
                
                <li class="trend-item">
                    <span class="trend-term">MoE</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">11 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">We</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">200 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">Long</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">14 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">Transformers</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">13 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">While</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">64 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">Announce Type</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">904 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">Abstract</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">904 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
            </ul>
        </div>

        <!-- Category Activity -->
        <div class="summary-card">
            <h3>Category Activity</h3>
            <div class="cat-bars">
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">AI &amp; LLMs</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 100%; background: #4F46E5;"></div>
                    </div>
                    <span class="cat-bar-count">904</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">Social Buzz</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 7%; background: #06B6D4;"></div>
                    </div>
                    <span class="cat-bar-count">61</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">Web3 &amp; DeFi</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 2%; background: #8B5CF6;"></div>
                    </div>
                    <span class="cat-bar-count">15</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">New Tools &amp; Apps</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 1%; background: #F59E0B;"></div>
                    </div>
                    <span class="cat-bar-count">7</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">Deals &amp; Acquisitions</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 0%; background: #EC4899;"></div>
                    </div>
                    <span class="cat-bar-count">3</span>
                </div>
                
                
                
                
                
                
            </div>
        </div>

        <!-- Stats -->
        <div class="summary-card">
            <h3>Today's Signal</h3>
            <div class="stat-grid">
                <div class="stat-item">
                    <div class="stat-value">990</div>
                    <div class="stat-label">Articles</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">19</div>
                    <div class="stat-label">Sources</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">197</div>
                    <div class="stat-label">Trending</div>
                </div>
                <div class="stat-item">
                    <span class="momentum-badge momentum-accelerating">
                        accelerating
                    </span>
                    <div class="stat-label">Momentum</div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Search -->
<input type="text" class="search-bar" placeholder="Search articles by title, source, or keyword..." aria-label="Search articles">

<!-- Trending Articles -->

<section>
    <div class="section-header">
        <h2>Trending Now</h2>
    </div>
    <div class="article-grid">
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/MachineLearning</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/MachineLearning/comments/1r6zq50/r_we_spent_a_decade_scaling_models_now_by_just/" target="_blank" rel="noopener">[R] We spent a decade scaling models. Now, by just shifting towards memory and continual learning, we can get to a human like AI or &#34;A-GEE-I&#34;</a>
    </h3>

    
    <p class="card-summary">Paper reference I’m curious to hear other perspectives. It increasingly feels like memory, not raw capability, is what still keeps AI below human intelligence. If memory, bandwidth, and energy efficiency keep improving, intelligence starts to look more like something being engineered rather than something strictly bounded by today’s scaling laws and optimization limits. Maybe progress doesn’t...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/MachineLearning</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/MachineLearning/comments/1r6yf4z/p_qwen35_parameter_size_rumored_400b/" target="_blank" rel="noopener">[P] Qwen3.5 parameter size rumored ~400B</a>
    </h3>

    
    <p class="card-summary">Some rumors suggest Qwen3.5 is ~400B with MoE. Curious how people feel about models at this scale. &amp;#32; submitted by &amp;#32; /u/heyjatin [link] &amp;#32; [comments]</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14039" target="_blank" rel="noopener">Geometry-Preserving Aggregation for Mixture-of-Experts Embedding Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14039v1 Announce Type: new Abstract: Mixture-of-Experts (MoE) embedding models combine expert outputs using weighted linear summation, implicitly assuming a linear subspace structure in the embedding space. This assumption is shown to be inconsistent with the geometry of expert...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Artificial Intelligence</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14301" target="_blank" rel="noopener">DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14301v1 Announce Type: cross Abstract: Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL)...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Artificial Intelligence</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2505.19645" target="_blank" rel="noopener">MoESD: Unveil Speculative Decoding&#39;s Potential for Accelerating Sparse MoE</a>
    </h3>

    
    <p class="card-summary">arXiv:2505.19645v4 Announce Type: replace-cross Abstract: Large Language Models (LLMs) have achieved remarkable success across many applications, with Mixture of Experts (MoE) models demonstrating great potential. Compared to traditional dense models, MoEs achieve better performance with less...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Artificial Intelligence</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2601.16905" target="_blank" rel="noopener">GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints</a>
    </h3>

    
    <p class="card-summary">arXiv:2601.16905v2 Announce Type: replace-cross Abstract: Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE&#39;s...</p>
    

    
</article>
        
    </div>
</section>


<!-- Latest by Date -->

<h3 class="date-label">February 17, 2026</h3>
<div class="article-grid">
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/MachineLearning</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/MachineLearning/comments/1r6zq50/r_we_spent_a_decade_scaling_models_now_by_just/" target="_blank" rel="noopener">[R] We spent a decade scaling models. Now, by just shifting towards memory and continual learning, we can get to a human like AI or &#34;A-GEE-I&#34;</a>
    </h3>

    
    <p class="card-summary">Paper reference I’m curious to hear other perspectives. It increasingly feels like memory, not raw capability, is what still keeps AI below human intelligence. If memory, bandwidth, and energy efficiency keep improving, intelligence starts to look more like something being engineered rather than something strictly bounded by today’s scaling laws and optimization limits. Maybe progress doesn’t...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinDesk</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">16m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.coindesk.com/markets/2026/02/17/defi-protocol-zerolend-shuts-down-after-3-years-citing-inactive-chains-and-hacks" target="_blank" rel="noopener">DeFi protocol ZeroLend shuts down after three years, citing inactive chains and hacks</a>
    </h3>

    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/StableDiffusion</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">19m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/StableDiffusion/comments/1r6zg4c/zimageturbo_simple_comparison_dora_vs_loha/" target="_blank" rel="noopener">Zimage-Turbo: Simple comparison: DoRA vs LoHA.</a>
    </h3>

    
    <p class="card-summary">Everything was trained on Onetrainer: CAME + REX, masked training, 26 images on dataset, 17 images for regularization, dim 32, alpha 12. RTX 4060ti 16gb + 64gb RAM. Zimage-Base LoHA (training blocks) (100 epochs):1h22m. Zimage-Base DoRA (training attn-mlp) (100 epochs):1h3m. Zimage-Base LoHA + Regularization + EMA (training attn-mlp) (100 epochs): 2h17m. I use a pretty aggresive training method...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/SideProject</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">22m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/SideProject/comments/1r6zej3/anyone_else_tired_of_reusing_the_same_prompts_in/" target="_blank" rel="noopener">Anyone else tired of reusing the same prompts in ChatGPT?</a>
    </h3>

    
    <p class="card-summary">I kept copy-pasting the same prompts into ChatGPT for different things every day and it started to get annoying. So I built a small Chrome extension for myself that lets me insert saved prompts into the chat input with one click. Cursor stays active so you can just continue typing. Built it using Codex, didn&#39;t overthink it, just scratched an itch. Posting here mostly to see if others have the...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/startups</span>
        <span class="card-category" style="background: #EC4899;">
            Deals &amp; Acquisitions
        </span>
        <span class="card-time">29m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/startups/comments/1r6z9tw/i_will_not_promote_do_you_think_learning_by_doing/" target="_blank" rel="noopener">I will not promote: do you think “learning by doing” actually works better than traditional classes?</a>
    </h3>

    
    <p class="card-summary">i’ve noticed i learn way more when i’m forced to build something real vs just studying theory. for example, working on small projects, pitching ideas, or launching something teaches you things no lecture really can, especially dealing with uncertainty, failure, and feedback. not saying theory is useless, but it feels incomplete without execution. curious how others see it, is learning by doing...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">The Block</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">36m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.theblock.co/post/390057/cryptos-300-billion-stablecoin-supply-is-increasingly-used-as-everyday-money-global-study-finds?utm_source=rss&amp;utm_medium=rss" target="_blank" rel="noopener">Crypto’s $300 billion stablecoin supply is increasingly used as ‘everyday money,’ global study finds</a>
    </h3>

    
    <p class="card-summary">Stablecoin use in everyday spending, cross-border work, and savings allocation is growing rapidly among crypto-savvy consumers.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinTelegraph</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">36m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://cointelegraph.com/news/bvnk-survey-finds-growing-use-of-stablecoins-for-income-and-everyday-payments?utm_source=rss_feed&amp;utm_medium=rss&amp;utm_campaign=rss_partner_inbound" target="_blank" rel="noopener">Stablecoins gain ground for paychecks and daily spending: BVNK report</a>
    </h3>

    
    <p class="card-summary">A global survey of 4,658 crypto users found 39% receive income in stablecoins and 27% use them for payments, with stronger adoption in emerging markets.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">Decrypt</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">48m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://decrypt.co/358264/steak-n-shake-bitcoin-lifted-sales-dramatically-nine-months" target="_blank" rel="noopener">Steak &#39;n Shake Says Bitcoin Has Lifted Sales &#39;Dramatically&#39; in 9 Months</a>
    </h3>

    
    <p class="card-summary">The fast-food chain says bitcoin payments have boosted sales, as it channels crypto receipts into a growing corporate reserve.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinTelegraph</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">51m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://cointelegraph.com/news/polygon-flips-ethereum-in-daily-transaction-fees?utm_source=rss_feed&amp;utm_medium=rss&amp;utm_campaign=rss_partner_inbound" target="_blank" rel="noopener">Polygon daily fees flip Ethereum amid prediction market boom</a>
    </h3>

    
    <p class="card-summary">Polygon briefly surpassed Ethereum in daily fees as Polymarket activity surged, highlighting shifting user demand toward Layer-2 networks.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/cryptocurrency</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/CryptoCurrency/comments/1r6yr4x/metaplanet_revenue_jumps_738_as_bitcoin_accounts/" target="_blank" rel="noopener">Metaplanet Revenue Jumps 738% as Bitcoin Accounts for 95% of Income</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/WiseChest8227 [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/OpenAI</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/OpenAI/comments/1r6yq53/saw_this_in_an_eacc_group_lol/" target="_blank" rel="noopener">saw this in an e/acc group lol</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/cobalt1137 [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/StableDiffusion</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/StableDiffusion/comments/1r6yn9w/ram_question/" target="_blank" rel="noopener">RAM Question</a>
    </h3>

    
    <p class="card-summary">I have 2 x 3600 32GB ram installed. So in total 64GB ram. Now i have a old 16GB 2666 mhz stick lying around. Installing it will give me 80GB in total. Considering difference in freequency is it worth it install the ram?? &amp;#32; submitted by &amp;#32; /u/witcherknight [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinTelegraph</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://cointelegraph.com/news/crypto-protocol-zerolend-shuts-down-no-longer-sustainable?utm_source=rss_feed&amp;utm_medium=rss&amp;utm_campaign=rss_partner_inbound" target="_blank" rel="noopener">DeFi lender ZeroLend shuts down, blames illiquid chains</a>
    </h3>

    
    <p class="card-summary">ZeroLend founder “Ryker” says several blockchains the lending protocol operates on are now “inactive,” leading to periods where it has operated at a loss.</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/MachineLearning</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/MachineLearning/comments/1r6yf4z/p_qwen35_parameter_size_rumored_400b/" target="_blank" rel="noopener">[P] Qwen3.5 parameter size rumored ~400B</a>
    </h3>

    
    <p class="card-summary">Some rumors suggest Qwen3.5 is ~400B with MoE. Curious how people feel about models at this scale. &amp;#32; submitted by &amp;#32; /u/heyjatin [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/OpenAI</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/OpenAI/comments/1r6yens/chatgpt_reminds_me_of_my_english_teacher/" target="_blank" rel="noopener">ChatGPT reminds me of my English teacher.</a>
    </h3>

    
    <p class="card-summary">For context, yes, it was the same chat session. And no, I didn’t know which policy I triggered. Probably copyright or harmful content, but since there was zero transparency, we’ll never know. In other words, the model was only allowed to analyze the refusal, not mention the actual refusal. &amp;#32; submitted by &amp;#32; /u/arlilo [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/StableDiffusion</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/StableDiffusion/comments/1r6y7gk/made_a_realism_luxury_fashion_portraits_lora_for/" target="_blank" rel="noopener">Made a realism luxury fashion portraits LoRA for Z-Image Turbo.</a>
    </h3>

    
    <p class="card-summary">I trained it on a bunch of high-quality images (most of them by Tamara Williams) because I wanted consistent lighting and that fashion/beauty photography feel. It seems to do really nice close-up portraits and magazine-style images. If anyone tries it or just looks at the samples — what do you think about it? Link...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/startups</span>
        <span class="card-category" style="background: #EC4899;">
            Deals &amp; Acquisitions
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/startups/comments/1r6y5xr/delaware_corp_documents_i_will_not_promote/" target="_blank" rel="noopener">Delaware corp documents (I will not promote)</a>
    </h3>

    
    <p class="card-summary">Hello all! Just curious. I incorporated in the beautiful state of Delaware about a week ago, but I have not received my incorporation documents nor my letter of good standing (as I also requested it). I requested my documents to be sent via USPS. What has been your guys&#39; experience? How long after you filed, you received your documents? Thanks! &amp;#32; submitted by &amp;#32; /u/Livid-Cat-5056 [link]...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/cryptocurrency</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/CryptoCurrency/comments/1r6y51d/crypto_scammers_have_gone_offline_now_phishing/" target="_blank" rel="noopener">Crypto scammers have gone offline, now phishing comes straight to your real mailbox</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/davideownzall [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/ethereum</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/ethereum/comments/1r6y4bq/daily_general_discussion_february_17_2026/" target="_blank" rel="noopener">Daily General Discussion February 17, 2026</a>
    </h3>

    
    <p class="card-summary">Welcome to the Daily General Discussion on r/ethereum https://imgur.com/3y7vezP Bookmarking this link will always bring you to the current daily: https://old.reddit.com/r/ethereum/about/sticky/?num=2 Please use this thread to discuss Ethereum topics, news, events, and even price! Price discussion posted elsewhere in the subreddit will continue to be removed. As always, be constructive. -...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/cryptocurrency</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/CryptoCurrency/comments/1r6y33y/steak_shake_said_near_exact_the_same_thing_last/" target="_blank" rel="noopener">Steak &amp;amp; Shake said near exact the same thing last month that you keep posting.. where’s the proof?</a>
    </h3>

    
    <p class="card-summary">The last 24 hours there’s multiple posts in this subreddit stating the same thing as above basically “Nine months ago today, Steak n Shake launched its burger-to-Bitcoin transformation when we started accepting bitcoin payments. Our same-store sales have risen dramatically ever since. Bitcoin payments for Steak n Shake burgers go into our Strategic Bitcoin Reserve, which then funds Bitcoin bonus...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/SideProject</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/SideProject/comments/1r6y1zc/i_hated_paying_for_cloudrenders_so_i_built_my_own/" target="_blank" rel="noopener">I hated paying for cloud-renders, so I built my own offline visualizer for beatmakers.</a>
    </h3>

    
    <p class="card-summary">Hey everyone, I was getting super frustrated with how hard it is to make good visuals for my music (Spotify Canvases, YouTube beats, etc.). After Effects takes hours to render, and cloud-based subscription sites are expensive and take forever to upload/download. So for my side project, I built Synthey. It’s a standalone offline visualizer for Mac and Windows that uses your local GPU. How it...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/ChatGPT</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/ChatGPT/comments/1r6xwsn/no_fluff/" target="_blank" rel="noopener">“No fluff”</a>
    </h3>

    
    <p class="card-summary">So sick of the nonsense ChatGPT has been spitting lately. I just want answers, not nonsense garbage after every request I send. &amp;#32; submitted by &amp;#32; /u/InternalMurkyxD [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/OpenAI</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/OpenAI/comments/1r6xwii/i_owe_the_its_gotten_worse_crowd_an_apology/" target="_blank" rel="noopener">I owe the &#34;it&#39;s gotten worse&#34; crowd an apology regarding ChatGPT 5.2</a>
    </h3>

    
    <p class="card-summary">Repost because the mods thought it was a good idea to delete today&#39;s top r/OpenAI post without any warning or message. https://www.reddit.com/r/OpenAI/comments/1r6cki1/i_owe_the_its_gotten_worse_crowd_an_apology/ In the past, I repeatedly found it amusing when people complained that ChatGPT had become too &amp;quot;critical&amp;quot; or &amp;quot;lazy.&amp;quot; I thought - and frequently commented - that it was...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/StableDiffusion</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/StableDiffusion/comments/1r6xu4x/ltx2_oom/" target="_blank" rel="noopener">LTX2 OOM</a>
    </h3>

    
    <p class="card-summary">I am running into an issue where I run a workflow, I get an out-of-memory error. Then I run it again, with the exact same settings, and it runs fine. It’s frustrating because it is so random when it works and when it doesn’t. Again same exact settings between runs. Has anyone else experienced this? Also I’m using a 3090 with 64gb ram using the dev fp8 version. &amp;#32; submitted by &amp;#32...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">Decrypt</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://decrypt.co/358258/kraken-wyomings-trump-accounts" target="_blank" rel="noopener">Kraken Backs &#39;Trump Accounts&#39; for Wyoming Newborns</a>
    </h3>

    
    <p class="card-summary">The move reflects a broader effort by crypto firms to lock in regulatory goodwill through long-term political and geographic alignment.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinDesk</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.coindesk.com/markets/2026/02/17/bofa-survey-shows-record-bearish-dollar-bets-here-s-what-it-means-for-bitcoin" target="_blank" rel="noopener">BofA survey flags dollar bearish bets at over a decade high. Here&#39;s what it means for bitcoin</a>
    </h3>

    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/LocalLLaMA</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/LocalLLaMA/comments/1r6xrjy/top_openclaw_alternatives_worth_actually_trying/" target="_blank" rel="noopener">Top OpenClaw Alternatives Worth Actually Trying (2026)</a>
    </h3>

    
    <p class="card-summary">The AI world moves fast, and OpenClaw&#39;s alternatives exist (security researchers&#39; words: shell access + plaintext API keys + unrestricted local exec) has quietly pushed a lot of developers to start looking around. Been evaluating OpenClaw alternatives for the past few weeks after the token leak stuff got bad enough that I couldn&#39;t ignore it anymore. Here&#39;s what I actually found: NanoClaw Same...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/artificial</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/artificial/comments/1r6xndz/openai_just_hired_the_openclaw_creator/" target="_blank" rel="noopener">OpenAI just hired the OpenClaw creator</a>
    </h3>

    
    <p class="card-summary">So the guy who built OpenClaw, originally called Clawdbot because it was literally named after Anthropic&#39;s Claude, just got hired by OpenAI. Not Anthropic. OpenAI. You can&#39;t make this stuff up. For those out of the loop: OpenClaw is that open-source AI assistant that actually DOES things instead of just talking about doing things. You run it on a Mac Mini or whatever, connect it to your...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinTelegraph</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://cointelegraph.com/news/extreme-fear-sentiment-suggests-market-inflection-point-matrixport?utm_source=rss_feed&amp;utm_medium=rss&amp;utm_campaign=rss_partner_inbound" target="_blank" rel="noopener">Crypto sentiment hits extreme fear as Matrixport flags possible bottom</a>
    </h3>

    
    <p class="card-summary">With Bitcoin sentiment at four-year lows, analysts have flagged historic oversold signals and potential seller exhaustion.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/cryptocurrency</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/CryptoCurrency/comments/1r6x8so/steak_n_shake_sales_jump_after_accepting_bitcoin/" target="_blank" rel="noopener">Steak ‘n Shake sales jump after accepting bitcoin payments</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/gdscrypto [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/SideProject</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/SideProject/comments/1r6x4tt/built_a_tool_that_gets_usable_ad_creatives_down/" target="_blank" rel="noopener">Built a tool that gets usable ad creatives down to 20 cents</a>
    </h3>

    
    <p class="card-summary">Built ugcjam.com because ad production speed was killing iteration. Now our workflow averages around 20 cents per usable creative. What changed: - we stopped rationing ideas - we test far more hooks and angles - we launch faster with less budget risk Flow is simple: - script → actor → ad - talking-head + product/demo formats - rapid variations - 30+ language localization Pricing currently: $1 for...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/SideProject</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/SideProject/comments/1r6wz3s/why_stage_locking_should_be_the_default_for/" target="_blank" rel="noopener">Why Stage Locking Should Be the Default for Freelancers (Not an “Extra Feature”)</a>
    </h3>

    
    <p class="card-summary">Most late payments and awkward follow-ups don’t happen because clients are difficult. They happen because the structure is weak. We start with a loose scope, do the work, send one big invoice at the end, and hope for the best. When payment gets delayed, we’ve already delivered everything, which means we’ve lost leverage. The real pain point isn’t just unpaid invoices. It’s doing great work and...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13263" target="_blank" rel="noopener">Multimodal Consistency-Guided Reference-Free Data Selection for ASR Accent Adaptation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13263v1 Announce Type: new Abstract: Automatic speech recognition (ASR) systems often degrade on accented speech because acoustic-phonetic and prosodic shifts induce a mismatch to training data, making labeled accent adaptation costly. However, common pseudo-label selection heuristics...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13452" target="_blank" rel="noopener">LLM-Powered Automatic Translation and Urgency in Crisis Scenarios</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13452v1 Announce Type: new Abstract: Large language models (LLMs) are increasingly proposed for crisis preparedness and response, particularly for multilingual communication. However, their suitability for high-stakes crisis contexts remains insufficiently evaluated. This work examines...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13455" target="_blank" rel="noopener">Using Machine Learning to Enhance the Detection of Obfuscated Abusive Words in Swahili: A Focus on Child Safety</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13455v1 Announce Type: new Abstract: The rise of digital technology has dramatically increased the potential for cyberbullying and online abuse, necessitating enhanced measures for detection and prevention, especially among children. This study focuses on detecting abusive obfuscated...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13466" target="_blank" rel="noopener">Language Model Memory and Memory Models for Language</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13466v1 Announce Type: new Abstract: The ability of machine learning models to store input information in hidden layer vector embeddings, analogous to the concept of `memory&#39;, is widely employed but not well characterized. We find that language model embeddings typically contain...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13504" target="_blank" rel="noopener">From Perceptions To Evidence: Detecting AI-Generated Content In Turkish News Media With A Fine-Tuned Bert Classifier</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13504v1 Announce Type: new Abstract: The rapid integration of large language models into newsroom workflows has raised urgent questions about the prevalence of AI-generated content in online media. While computational studies have begun to quantify this phenomenon in English-language...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13517" target="_blank" rel="noopener">Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13517v1 Announce Type: new Abstract: Large language models (LLMs) have demonstrated impressive reasoning capabilities by scaling test-time compute via long Chain-of-Thought (CoT). However, recent findings suggest that raw token counts are unreliable proxies for reasoning quality...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13540" target="_blank" rel="noopener">On Calibration of Large Language Models: From Response To Capability</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13540v1 Announce Type: new Abstract: Large language models (LLMs) are widely deployed as general-purpose problem solvers, making accurate confidence estimation critical for reliable use. Prior work on LLM calibration largely focuses on response-level confidence, which estimates the...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13551" target="_blank" rel="noopener">Small Reward Models via Backward Inference</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13551v1 Announce Type: new Abstract: Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13567" target="_blank" rel="noopener">DistillLens: Symmetric Knowledge Distillation Through Logit Lens</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13567v1 Announce Type: new Abstract: Standard Knowledge Distillation (KD) compresses Large Language Models (LLMs) by optimizing final outputs, yet it typically treats the teacher&#39;s intermediate layer&#39;s thought process as a black box. While feature-based distillation attempts to bridge...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13571" target="_blank" rel="noopener">LLM-Confidence Reranker: A Training-Free Approach for Enhancing Retrieval-Augmented Generation Systems</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13571v1 Announce Type: new Abstract: Large language models (LLMs) have revolutionized natural language processing, yet hallucinations in knowledge-intensive tasks remain a critical challenge. Retrieval-augmented generation (RAG) addresses this by integrating external knowledge, but its...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13575" target="_blank" rel="noopener">Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13575v1 Announce Type: new Abstract: Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13701" target="_blank" rel="noopener">Metaphors&#39; journeys across time and genre: tracking the evolution of literary metaphors with temporal embeddings</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13701v1 Announce Type: new Abstract: Metaphors are a distinctive feature of literary language, yet they remain less studied experimentally than everyday metaphors. Moreover, previous psycholinguistic and computational approaches overlooked the temporal dimension, although many literary...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13713" target="_blank" rel="noopener">On Theoretically-Driven LLM Agents for Multi-Dimensional Discourse Analysis</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13713v1 Announce Type: new Abstract: Identifying the strategic uses of reformulation in discourse remains a key challenge for computational argumentation. While LLMs can detect surface-level similarity, they often fail to capture the pragmatic functions of rephrasing, such as its role...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13748" target="_blank" rel="noopener">RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13748v1 Announce Type: new Abstract: Multimedia Event Extraction (MEE) aims to identify events and their arguments from documents that contain both text and images. It requires grounding event semantics across different modalities. Progress in MEE is limited by the lack of annotated...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13790" target="_blank" rel="noopener">How Do Lexical Senses Correspond Between Spoken German and German Sign Language?</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13790v1 Announce Type: new Abstract: Sign language lexicographers construct bilingual dictionaries by establishing word-to-sign mappings, where polysemous and homonymous words corresponding to different signs across contexts are often underrepresented. A usage-based approach examining...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13793" target="_blank" rel="noopener">OMGs: A multi-agent system supporting MDT decision-making across the ovarian tumour care continuum</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13793v1 Announce Type: new Abstract: Ovarian tumour management has increasingly relied on multidisciplinary tumour board (MDT) deliberation to address treatment complexity and disease heterogeneity. However, most patients worldwide lack access to timely expert consensus, particularly in...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13816" target="_blank" rel="noopener">The acquisition of English irregular inflections by Yemeni L1 Arabic learners: A Universal Grammar approach</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13816v1 Announce Type: new Abstract: This study examines the acquisition of English irregular inflections by Yemeni learners of English as a second language (L2), utilizing a Universal Grammar (UG) approach. Within the UG approach, the study considers Feature Reassembly Hypothesis (FRH)...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13832" target="_blank" rel="noopener">Beyond Words: Evaluating and Bridging Epistemic Divergence in User-Agent Interaction via Theory of Mind</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13832v1 Announce Type: new Abstract: Large Language Models (LLMs) have developed rapidly and are widely applied to both general-purpose and professional tasks to assist human users. However, they still struggle to comprehend and respond to the true user needs when intentions and...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13836" target="_blank" rel="noopener">Speculative Decoding with a Speculative Vocabulary</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13836v1 Announce Type: new Abstract: Speculative decoding has rapidly emerged as a leading approach for accelerating language model (LM) inference, as it offers substantial speedups while yielding identical outputs. This relies upon a small draft model, tasked with predicting the outputs...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13840" target="_blank" rel="noopener">PrivAct: Internalizing Contextual Privacy Preservation via Multi-Agent Preference Training</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13840v1 Announce Type: new Abstract: Large language model (LLM) agents are increasingly deployed in personalized tasks involving sensitive, context-dependent information, where privacy violations may arise in agents&#39; action due to the implicitness of contextual privacy. Existing...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13860" target="_blank" rel="noopener">Tutoring Large Language Models to be Domain-adaptive, Precise, and Safe</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13860v1 Announce Type: new Abstract: The overarching research direction of this work is the development of a &#39;&#39;Responsible Intelligence&#39;&#39; framework designed to reconcile the immense generative power of Large Language Models (LLMs) with the stringent requirements of real-world deployment...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13867" target="_blank" rel="noopener">Bridging the Multilingual Safety Divide: Efficient, Culturally-Aware Alignment for Global South Languages</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13867v1 Announce Type: new Abstract: Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13870" target="_blank" rel="noopener">ADAB: Arabic Dataset for Automated Politeness Benchmarking -- A Large-Scale Resource for Computational Sociopragmatics</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13870v1 Announce Type: new Abstract: The growing importance of culturally-aware natural language processing systems has led to an increasing demand for resources that capture sociopragmatic phenomena across diverse languages. Nevertheless, Arabic-language resources for politeness...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13890" target="_blank" rel="noopener">Evaluating Prompt Engineering Techniques for RAG in Small Language Models: A Multi-Hop QA Approach</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13890v1 Announce Type: new Abstract: Retrieval Augmented Generation (RAG) is a powerful approach for enhancing the factual grounding of language models by integrating external knowledge. While widely studied for large language models, the optimization of RAG for Small Language Models...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13905" target="_blank" rel="noopener">Pre-Editorial Normalization for Automatically Transcribed Medieval Manuscripts in Old French and Latin</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13905v1 Announce Type: new Abstract: Recent advances in Automatic Text Recognition (ATR) have improved access to historical archives, yet a methodological divide persists between palaeographic transcriptions and normalized digital editions. While ATR models trained on more...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13964" target="_blank" rel="noopener">HLE-Verified: A Systematic Verification and Structured Revision of Humanity&#39;s Last Exam</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13964v1 Announce Type: new Abstract: Humanity&#39;s Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13979" target="_blank" rel="noopener">Chain-of-Thought Reasoning with Large Language Models for Clinical Alzheimer&#39;s Disease Assessment and Diagnosis</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13979v1 Announce Type: new Abstract: Alzheimer&#39;s disease (AD) has become a prevalent neurodegenerative disease worldwide. Traditional diagnosis still relies heavily on medical imaging and clinical assessment by physicians, which is often time-consuming and resource-intensive in terms of...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14002" target="_blank" rel="noopener">The Sufficiency-Conciseness Trade-off in LLM Self-Explanation from an Information Bottleneck Perspective</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14002v1 Announce Type: new Abstract: Large Language Models increasingly rely on self-explanations, such as chain of thought reasoning, to improve performance on multi step question answering. While these explanations enhance accuracy, they are often verbose and costly to generate...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14009" target="_blank" rel="noopener">Named Entity Recognition for Payment Data Using NLP</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14009v1 Announce Type: new Abstract: Named Entity Recognition (NER) has emerged as a critical component in automating financial transaction processing, particularly in extracting structured information from unstructured payment data. This paper presents a comprehensive analysis of...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14028" target="_blank" rel="noopener">GRRM: Group Relative Reward Modeling for Machine Translation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14028v1 Announce Type: new Abstract: While Group Relative Policy Optimization (GRPO) offers a powerful framework for LLM post-training, its effectiveness in open-ended domains like Machine Translation hinges on accurate intra-group ranking. We identify that standard Scalar Quality...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14039" target="_blank" rel="noopener">Geometry-Preserving Aggregation for Mixture-of-Experts Embedding Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14039v1 Announce Type: new Abstract: Mixture-of-Experts (MoE) embedding models combine expert outputs using weighted linear summation, implicitly assuming a linear subspace structure in the embedding space. This assumption is shown to be inconsistent with the geometry of expert...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14044" target="_blank" rel="noopener">Context Shapes LLMs Retrieval-Augmented Fact-Checking Effectiveness</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14044v1 Announce Type: new Abstract: Large language models (LLMs) show strong reasoning abilities across diverse tasks, yet their performance on extended contexts remains inconsistent. While prior research has emphasized mid-context degradation in question answering, this study examines...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14054" target="_blank" rel="noopener">LogitsCoder: Towards Efficient Chain-of-Thought Path Search via Logits Preference Decoding for Code Generation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14054v1 Announce Type: new Abstract: Code generation remains a challenging task that requires precise and structured reasoning. Existing Test Time Scaling (TTS) methods, including structured tree search, have made progress in exploring reasoning paths but still face two major challenges...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14060" target="_blank" rel="noopener">LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14060v1 Announce Type: new Abstract: We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14062" target="_blank" rel="noopener">From Scarcity to Scale: A Release-Level Analysis of the Pashto Common Voice Dataset</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14062v1 Announce Type: new Abstract: Large, openly licensed speech datasets are essential for building automatic speech recognition (ASR) systems, yet many widely spoken languages remain underrepresented in public resources. Pashto, spoken by more than 60 million people, has historically...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14069" target="_blank" rel="noopener">Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14069v1 Announce Type: new Abstract: Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14073" target="_blank" rel="noopener">Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14073v1 Announce Type: new Abstract: Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14077" target="_blank" rel="noopener">GTS: Inference-Time Scaling of Latent Reasoning with a Learnable Gaussian Thought Sampler</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14077v1 Announce Type: new Abstract: Inference-time scaling (ITS) in latent reasoning models typically introduces stochasticity through heuristic perturbations, such as dropout or fixed Gaussian noise. While these methods increase trajectory diversity, their exploration behavior is not...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14080" target="_blank" rel="noopener">Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14080v1 Announce Type: new Abstract: Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14081" target="_blank" rel="noopener">CCiV: A Benchmark for Structure, Rhythm and Quality in LLM-Generated Chinese \textit{Ci} Poetry</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14081v1 Announce Type: new Abstract: The generation of classical Chinese \textit{Ci} poetry, a form demanding a sophisticated blend of structural rigidity, rhythmic harmony, and artistic quality, poses a significant challenge for large language models (LLMs). To systematically evaluate...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14100" target="_blank" rel="noopener">Character-aware Transformers Learn an Irregular Morphological Pattern Yet None Generalize Like Humans</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14100v1 Announce Type: new Abstract: Whether neural networks can serve as cognitive models of morphological learning remains an open question. Recent work has shown that encoder-decoder models can acquire irregular patterns, but evidence that they generalize these patterns like humans is...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14158" target="_blank" rel="noopener">A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14158v1 Announce Type: new Abstract: Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14162" target="_blank" rel="noopener">Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14162v1 Announce Type: new Abstract: Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14188" target="_blank" rel="noopener">GPT-5 vs Other LLMs in Long Short-Context Performance</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14188v1 Announce Type: new Abstract: With the significant expansion of the context window in Large Language Models (LLMs), these models are theoretically capable of processing millions of tokens in a single pass. However, research indicates a significant gap between this theoretical...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14189" target="_blank" rel="noopener">Knowing When Not to Answer: Abstention-Aware Scientific Reasoning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14189v1 Announce Type: new Abstract: Large language models are increasingly used to answer and verify scientific claims, yet existing evaluations typically assume that a model must always produce a definitive answer. In scientific settings, however, unsupported or uncertain conclusions...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14238" target="_blank" rel="noopener">We can still parse using syntactic rules</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14238v1 Announce Type: new Abstract: This research introduces a new parsing approach, based on earlier syntactic work on context free grammar (CFG) and generalized phrase structure grammar (GPSG). The approach comprises both a new parsing algorithm and a set of syntactic rules and...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14257" target="_blank" rel="noopener">AD-Bench: A Real-World, Trajectory-Aware Advertising Analytics Benchmark for LLM Agents</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14257v1 Announce Type: new Abstract: While Large Language Model (LLM) agents have achieved remarkable progress in complex reasoning tasks, evaluating their performance in real-world environments has become a critical problem. Current benchmarks, however, are largely restricted to...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14259" target="_blank" rel="noopener">Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14259v1 Announce Type: new Abstract: We propose a geometric taxonomy of large language model hallucinations based on observable signatures in token embedding cluster structure. By analyzing the static embedding spaces of 11 transformer models spanning encoder (BERT, RoBERTa, ELECTRA...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14265" target="_blank" rel="noopener">STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14265v1 Announce Type: new Abstract: Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14299" target="_blank" rel="noopener">Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14299v1 Announce Type: new Abstract: As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14367" target="_blank" rel="noopener">InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14367v1 Announce Type: new Abstract: The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14386" target="_blank" rel="noopener">Beyond Token-Level Policy Gradients for Complex Reasoning with Large Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14386v1 Announce Type: new Abstract: Existing policy-gradient methods for auto-regressive language models typically select subsequent tokens one at a time as actions in the policy. While effective for many generation tasks, such an approach may not fully capture the structure of complex...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14406" target="_blank" rel="noopener">TruthStance: An Annotated Dataset of Conversations on Truth Social</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14406v1 Announce Type: new Abstract: Argument mining and stance detection are central to understanding how opinions are formed and contested in online discourse. However, most publicly available resources focus on mainstream platforms such as Twitter and Reddit, leaving conversational...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14419" target="_blank" rel="noopener">WavePhaseNet: A DFT-Based Method for Constructing Semantic Conceptual Hierarchy Structures (SCHS)</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14419v1 Announce Type: new Abstract: This paper reformulates Transformer/Attention mechanisms in Large Language Models (LLMs) through measure theory and frequency analysis, theoretically demonstrating that hallucination is an inevitable structural limitation. The embedding space...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14428" target="_blank" rel="noopener">LLM-Guided Knowledge Distillation for Temporal Knowledge Graph Reasoning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14428v1 Announce Type: new Abstract: Temporal knowledge graphs (TKGs) support reasoning over time-evolving facts, yet state-of-the-art models are often computationally heavy and costly to deploy. Existing compression and distillation techniques are largely designed for static graphs...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14466" target="_blank" rel="noopener">Robust Bias Evaluation with FilBBQ: A Filipino Bias Benchmark for Question-Answering Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14466v1 Announce Type: new Abstract: With natural language generation becoming a popular use case for language models, the Bias Benchmark for Question-Answering (BBQ) has grown to be an important benchmark format for evaluating stereotypical associations exhibited by generative models...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14469" target="_blank" rel="noopener">Measuring and Mitigating Post-hoc Rationalization in Reverse Chain-of-Thought Generation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14469v1 Announce Type: new Abstract: Reverse Chain-of-Thought Generation (RCG) synthesizes reasoning traces from query-answer pairs, but runs the risk of producing post-hoc rationalizations: when models can see the answer during generation, the answer serves as a cognitive anchor that...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14470" target="_blank" rel="noopener">HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14470v1 Announce Type: new Abstract: Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14488" target="_blank" rel="noopener">BETA-Labeling for Multilingual Dataset Construction in Low-Resource IR</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14488v1 Announce Type: new Abstract: IR in low-resource languages remains limited by the scarcity of high-quality, task-specific annotated datasets. Manual annotation is expensive and difficult to scale, while using large language models (LLMs) as automated annotators introduces concerns...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14492" target="_blank" rel="noopener">Query as Anchor: Scenario-Adaptive User Representation via Large Language Model</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14492v1 Announce Type: new Abstract: Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14517" target="_blank" rel="noopener">Beyond Translation: Evaluating Mathematical Reasoning Capabilities of LLMs in Sinhala and Tamil</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14517v1 Announce Type: new Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning in English, but whether these capabilities reflect genuine multilingual reasoning or reliance on translation-based processing in low-resource languages like Sinhala and Tamil...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14536" target="_blank" rel="noopener">Explainable Token-level Noise Filtering for LLM Fine-tuning Datasets</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14536v1 Announce Type: new Abstract: Large Language Models (LLMs) have seen remarkable advancements, achieving state-of-the-art results in diverse applications. Fine-tuning, an important step for adapting LLMs to specific downstream tasks, typically involves further training on...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14564" target="_blank" rel="noopener">Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14564v1 Announce Type: new Abstract: Recently, Large Language Models (LLMs) have gained significant traction in medical domain, especially in developing a QA systems to Medical QA systems for enhancing access to healthcare in low-resourced settings. This paper compares five LLMs deployed...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14594" target="_blank" rel="noopener">The Wikidata Query Logs Dataset</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14594v1 Announce Type: new Abstract: We present the Wikidata Query Logs (WDQL) dataset, a dataset consisting of 200k question-query pairs over the Wikidata knowledge graph. It is over 6x larger than the largest existing Wikidata datasets of similar format without relying on...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14649" target="_blank" rel="noopener">GradMAP: Faster Layer Pruning with Gradient Metric and Projection Compensation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14649v1 Announce Type: new Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities, but their high computational costs limit their practical deployment. Recent studies reveal significant redundancy in LLMs layers, making layer pruning an active research topic. Layer...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14653" target="_blank" rel="noopener">Is Information Density Uniform when Utterances are Grounded on Perception and Discourse?</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14653v1 Announce Type: new Abstract: The Uniform Information Density (UID) hypothesis posits that speakers are subject to a communicative pressure to distribute information evenly within utterances, minimising surprisal variance. While this hypothesis has been tested empirically, prior...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14655" target="_blank" rel="noopener">Breaking Data Efficiency Dilemma: A Federated and Augmented Learning Framework For Alzheimer&#39;s Disease Detection via Speech</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14655v1 Announce Type: new Abstract: Early diagnosis of Alzheimer&#39;s Disease (AD) is crucial for delaying its progression. While AI-based speech detection is non-invasive and cost-effective, it faces a critical data efficiency dilemma due to medical data scarcity and privacy barriers...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14675" target="_blank" rel="noopener">Crowdsourcing Piedmontese to Test LLMs on Non-Standard Orthography</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14675v1 Announce Type: new Abstract: We present a crowdsourced dataset for Piedmontese, an endangered Romance language of northwestern Italy. The dataset comprises 145 Italian-Piedmontese parallel sentences derived from Flores+, with translations produced by speakers writing in their...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14743" target="_blank" rel="noopener">LLMStructBench: Benchmarking Large Language Model Structured Data Extraction</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14743v1 Announce Type: new Abstract: We present LLMStructBench, a novel benchmark for evaluating Large Language Models (LLMs) on extracting structured data and generating valid JavaScript Object Notation (JSON) outputs from natural-language text. Our open dataset comprises diverse...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14744" target="_blank" rel="noopener">Rethinking the Role of LLMs in Time Series Forecasting</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14744v1 Announce Type: new Abstract: Large language models (LLMs) have been introduced to time series forecasting (TSF) to incorporate contextual knowledge beyond numerical signals. However, existing studies question whether LLMs provide genuine benefits, often reporting comparable...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14749" target="_blank" rel="noopener">Cognitive networks reconstruct mindsets about STEM subjects and educational contexts in almost 1000 high-schoolers, University students and LLM-based digital twins</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14749v1 Announce Type: new Abstract: Attitudes toward STEM develop from the interaction of conceptual knowledge, educational experiences, and affect. Here we use cognitive network science to reconstruct group mindsets as behavioural forma mentis networks (BFMNs). In this case, nodes are...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14760" target="_blank" rel="noopener">Residual Connections and the Causal Shift: Uncovering a Structural Misalignment in Transformers</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14760v1 Announce Type: new Abstract: Large Language Models (LLMs) are trained with next-token prediction, implemented in autoregressive Transformers via causal masking for parallelism. This creates a subtle misalignment: residual connections tie activations to the current token, while...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14763" target="_blank" rel="noopener">Unlocking Reasoning Capability on Machine Translation in Large Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14763v1 Announce Type: new Abstract: Reasoning-oriented large language models (RLMs) achieve strong gains on tasks such as mathematics and coding by generating explicit intermediate reasoning. However, their impact on machine translation (MT) remains underexplored. We systematically...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14770" target="_blank" rel="noopener">Multi-Agent Comedy Club: Investigating Community Discussion Effects on LLM Humor Generation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14770v1 Announce Type: new Abstract: Prior work has explored multi-turn interaction and feedback for LLM writing, but evaluations still largely center on prompts and localized feedback, leaving persistent public reception in online communities underexamined. We test whether broadcast...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14777" target="_blank" rel="noopener">Emergently Misaligned Language Models Show Behavioral Self-Awareness That Shifts With Subsequent Realignment</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14777v1 Announce Type: new Abstract: Recent research has demonstrated that large language models (LLMs) fine-tuned on incorrect trivia question-answer pairs exhibit toxicity - a phenomenon later termed &#34;emergent misalignment&#34;. Moreover, research has shown that LLMs possess behavioral...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14778" target="_blank" rel="noopener">A Geometric Analysis of Small-sized Language Model Hallucinations</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14778v1 Announce Type: new Abstract: Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings. This work investigates hallucinations in small-sized LLMs through a geometric...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14798" target="_blank" rel="noopener">Overthinking Loops in Agents: A Structural Risk via MCP Tools</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14798v1 Announce Type: new Abstract: Tool-using LLM agents increasingly coordinate real workloads by selecting and chaining third-party tools based on text-visible metadata such as tool names, descriptions, and return messages. We show that this convenience creates a supply-chain attack...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14812" target="_blank" rel="noopener">Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14812v1 Announce Type: new Abstract: Physical commonsense reasoning represents a fundamental capability of human intelligence, enabling individuals to understand their environment, predict future events, and navigate physical spaces. Recent years have witnessed growing interest in...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14819" target="_blank" rel="noopener">Testimole-Conversational: A 30-Billion-Word Italian Discussion Board Corpus (1996-2024) for Language Modeling and Sociolinguistic Research</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14819v1 Announce Type: new Abstract: We present &#34;Testimole-conversational&#34; a massive collection of discussion boards messages in the Italian language. The large size of the corpus, more than 30B word-tokens (1996-2024), renders it an ideal dataset for native Italian Large Language...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14917" target="_blank" rel="noopener">BFS-PO: Best-First Search for Large Reasoning Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14917v1 Announce Type: new Abstract: Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14955" target="_blank" rel="noopener">Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14955v1 Announce Type: new Abstract: We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.14970" target="_blank" rel="noopener">Counterfactual Fairness Evaluation of LLM-Based Contact Center Agent Quality Assurance System</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.14970v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly deployed in contact-center Quality Assurance (QA) to automate agent performance evaluation and coaching feedback. While LLMs offer unprecedented scalability and speed, their reliance on web-scale training...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.15005" target="_blank" rel="noopener">Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.15005v1 Announce Type: new Abstract: News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user&#39;s underlying information needs from heterogeneous signals that often extend...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.15012" target="_blank" rel="noopener">Cold-Start Personalization via Training-Free Priors from Structured World Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.15012v1 Announce Type: new Abstract: Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.15013" target="_blank" rel="noopener">Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.15013v1 Announce Type: new Abstract: This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2504.18880" target="_blank" rel="noopener">Reshaping MOFs text mining with a dynamic multi-agents framework of large language model</a>
    </h3>

    
    <p class="card-summary">arXiv:2504.18880v3 Announce Type: cross Abstract: Accurately identifying the synthesis conditions of metal-organic frameworks (MOFs) is essential for guiding experimental design, yet remains challenging because relevant information in the literature is often scattered, inconsistent, and difficult...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13218" target="_blank" rel="noopener">Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13218v1 Announce Type: cross Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.13224" target="_blank" rel="noopener">A Geometric Taxonomy of Hallucinations in LLMs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.13224v1 Announce Type: cross Abstract: The term &#34;hallucination&#34; in large language models conflates distinct phenomena with different geometric signatures in embedding space. We propose a taxonomy identifying three types: unfaithfulness (failure to engage with provided context)...</p>
    

    
</article>
    
</div>


    </main>

    <footer class="site-footer">
        <div class="container">
            <p>Last updated: February 17, 2026 at 07:36 UTC</p>
            <p>990 articles from 19 sources</p>
            <p>Daily Signal Feed &mdash; AI, Web3 &amp; Emerging Tech Aggregator</p>
        </div>
    </footer>

    <script>
    // Client-side search filter
    document.addEventListener('DOMContentLoaded', function() {
        const searchBar = document.querySelector('.search-bar');
        if (searchBar) {
            searchBar.addEventListener('input', function(e) {
                const query = e.target.value.toLowerCase();
                const cards = document.querySelectorAll('.article-card, .archive-item');
                cards.forEach(function(card) {
                    const text = card.textContent.toLowerCase();
                    card.style.display = text.includes(query) ? '' : 'none';
                });
            });
        }
    });
    </script>
</body>
</html>