<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily Signal Feed — AI, Web3 & Emerging Trends</title>
    <link rel="stylesheet" href="/daily-signal-feed/css/style.css">
    <meta name="description" content="AI, Web3, and emerging tech news aggregated from 50+ sources with trend detection.">
</head>
<body>
    <header class="site-header">
        <div class="container header-inner">
            <a href="/daily-signal-feed/" class="site-logo">
                <h1>Daily Signal Feed</h1>
                <span class="tagline">AI &bull; Web3 &bull; Emerging Trends</span>
            </a>
            <nav class="main-nav">
                <a href="/daily-signal-feed/" class="active">Home</a>
                
                <a href="/daily-signal-feed/category/ai-llms.html"
                   class="">
                    AI &amp; LLMs (592)
                </a>
                
                <a href="/daily-signal-feed/category/web3-defi.html"
                   class="">
                    Web3 &amp; DeFi (22)
                </a>
                
                <a href="/daily-signal-feed/category/deals.html"
                   class="">
                    Deals &amp; Acquisitions (6)
                </a>
                
                <a href="/daily-signal-feed/category/new-tools.html"
                   class="">
                    New Tools &amp; Apps (6)
                </a>
                
                <a href="/daily-signal-feed/category/regulation.html"
                   class="">
                    Regulation &amp; Policy
                </a>
                
                <a href="/daily-signal-feed/category/social-buzz.html"
                   class="">
                    Social Buzz (52)
                </a>
                
                <a href="/daily-signal-feed/category/emerging-trends.html"
                   class="">
                    Emerging Trends
                </a>
                
                <a href="/daily-signal-feed/archive.html" class="">Archive</a>
            </nav>
        </div>
    </header>

    <main class="container">
        
<!-- Executive Summary -->
<section class="exec-summary">
    <h2>Signal Summary</h2>

    <div class="summary-grid">
        <!-- Trending Topics -->
        <div class="summary-card">
            <h3>Trending Now</h3>
            <ul class="trend-list">
                
                <li class="trend-item">
                    <span class="trend-term">Reinforcement Learning</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">19 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">End</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">9 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">Vision</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">15 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">RL</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">55 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">CLI</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">7 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">Prior</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">8 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
                <li class="trend-item">
                    <span class="trend-term">However</span>
                    <span class="trend-meta">
                        <span class="trend-mentions">93 mentions</span>
                        <span class="trend-direction up">
                            &uarr;
                        </span>
                    </span>
                </li>
                
            </ul>
        </div>

        <!-- Category Activity -->
        <div class="summary-card">
            <h3>Category Activity</h3>
            <div class="cat-bars">
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">AI &amp; LLMs</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 100%; background: #4F46E5;"></div>
                    </div>
                    <span class="cat-bar-count">592</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">Social Buzz</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 9%; background: #06B6D4;"></div>
                    </div>
                    <span class="cat-bar-count">52</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">Web3 &amp; DeFi</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 4%; background: #8B5CF6;"></div>
                    </div>
                    <span class="cat-bar-count">22</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">Deals &amp; Acquisitions</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 1%; background: #EC4899;"></div>
                    </div>
                    <span class="cat-bar-count">6</span>
                </div>
                
                
                
                <div class="cat-bar">
                    <span class="cat-bar-label">New Tools &amp; Apps</span>
                    <div class="cat-bar-track">
                        <div class="cat-bar-fill" style="width: 1%; background: #F59E0B;"></div>
                    </div>
                    <span class="cat-bar-count">6</span>
                </div>
                
                
                
                
                
                
            </div>
        </div>

        <!-- Stats -->
        <div class="summary-card">
            <h3>Today's Signal</h3>
            <div class="stat-grid">
                <div class="stat-item">
                    <div class="stat-value">678</div>
                    <div class="stat-label">Articles</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">21</div>
                    <div class="stat-label">Sources</div>
                </div>
                <div class="stat-item">
                    <div class="stat-value">24</div>
                    <div class="stat-label">Trending</div>
                </div>
                <div class="stat-item">
                    <span class="momentum-badge momentum-accelerating">
                        accelerating
                    </span>
                    <div class="stat-label">Momentum</div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Search -->
<input type="text" class="search-bar" placeholder="Search articles by title, source, or keyword..." aria-label="Search articles">

<!-- Trending Articles -->

<section>
    <div class="section-header">
        <h2>Trending Now</h2>
    </div>
    <div class="article-grid">
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/OpenAI</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">10m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/OpenAI/comments/1r2n41e/quitgpt_campaign_wants_you_to_ditch_chatgpt_over/" target="_blank" rel="noopener">&#39;QuitGPT&#39; Campaign Wants You to Ditch ChatGPT Over OpenAI&#39;s Ties to Trump, ICE</a>
    </h3>

    
    <p class="card-summary">A growing movement is calling for users to cancel their ChatGPT subscriptions after reports surfaced detailing OpenAI’s deepening ties to the Trump administration. The campaign highlights a $25 million donation to a pro-Trump super PAC by OpenAI President Greg Brockman and revelations that ICE is using GPT-4 for surveillance and resume screening. &amp;#32; submitted by &amp;#32; /u/EchoOfOppenheimer...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">CoinDesk</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">56m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.coindesk.com/markets/2026/02/12/trump-linked-wlfi-s-zak-folkman-teases-forex-platform-at-consensus-hong-kong" target="_blank" rel="noopener">Trump-linked WLFI&#39;s Zak Folkman teases forex platform at Consensus Hong Kong</a>
    </h3>

    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11044" target="_blank" rel="noopener">Language Model Inversion through End-to-End Differentiation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11044v1 Announce Type: new Abstract: Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs. That is, given a LM and a desirable target output sequence of tokens, determining what input prompts would yield the target output remains an open...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11089" target="_blank" rel="noopener">DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11089v1 Announce Type: new Abstract: In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \emph{data recipe}, which comprises a data processing pipeline to transform...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Artificial Intelligence</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10458" target="_blank" rel="noopener">Found-RL: foundation model-enhanced reinforcement learning for autonomous driving</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10458v1 Announce Type: new Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly...</p>
    

    
</article>
        
            <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Artificial Intelligence</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10699" target="_blank" rel="noopener">Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10699v1 Announce Type: new Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental...</p>
    

    
</article>
        
    </div>
</section>


<!-- Latest by Date -->

<h3 class="date-label">February 12, 2026</h3>
<div class="article-grid">
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/cryptocurrency</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/CryptoCurrency/comments/1r2n9e5/layerzero_crosschain_ecosystem_analysis_159m/" target="_blank" rel="noopener">LayerZero Cross-Chain Ecosystem Analysis: 159M Messages, $225B Volume &amp;amp; 733+ OFTs Explained</a>
    </h3>

    
    <p class="card-summary">LayerZero has processed 159M+ cross-chain messages across 168 active chains, making it one of the most used interoperability protocols in Web3. The network has facilitated $225B+ in total value transferred, acting as a major liquidity rail for multichain capital flows. Over 701+ applications and 733+ OFTs are live, powering omnichain token movement. Stargate leads with 57.6M messages, while...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/StableDiffusion</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">8m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/StableDiffusion/comments/1r2n55e/anybody_else_tried_this_my_results_were_kleinlike/" target="_blank" rel="noopener">Anybody else tried this? My results were Klein-like.</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/rinkusonic [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">r/OpenAI</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">10m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/OpenAI/comments/1r2n41e/quitgpt_campaign_wants_you_to_ditch_chatgpt_over/" target="_blank" rel="noopener">&#39;QuitGPT&#39; Campaign Wants You to Ditch ChatGPT Over OpenAI&#39;s Ties to Trump, ICE</a>
    </h3>

    
    <p class="card-summary">A growing movement is calling for users to cancel their ChatGPT subscriptions after reports surfaced detailing OpenAI’s deepening ties to the Trump administration. The campaign highlights a $25 million donation to a pro-Trump super PAC by OpenAI President Greg Brockman and revelations that ICE is using GPT-4 for surveillance and resume screening. &amp;#32; submitted by &amp;#32; /u/EchoOfOppenheimer...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/defi</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">12m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/defi/comments/1r2n2tp/where_to_swap_into_xmrmonero_privately/" target="_blank" rel="noopener">Where to swap into XMR/Monero privately?</a>
    </h3>

    
    <p class="card-summary">I’m looking for an exchange or swap service that supports Monero and actually respects user privacy. Not trying to do anything sketchy. I just want to avoid KYC and keep my personal information to myself. Most places I’ve checked either don’t support XMR anymore or require full verification. If anyone knows of a privacy-friendly service that still works, I’d appreciate the help. &amp;#32; submitted...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/OpenAI</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">17m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/OpenAI/comments/1r2n0ec/account_now_thinks_it_requires_social_sign_in/" target="_blank" rel="noopener">Account now thinks it requires social sign in</a>
    </h3>

    
    <p class="card-summary">I&#39;ve just created a new account (as my previous account was set up with Apple and a private relay email address, and I wanted to get away from social sign in), and set it up manually with an email address and password. I set up MFA and a passkey: everything is going good. I use 1Password, and remember creating the initial entry with the email address and password, and updating it with the Passkey...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/StableDiffusion</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">18m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/StableDiffusion/comments/1r2mzni/is_16gb_vram_5080_enough_to_train_models_like/" target="_blank" rel="noopener">Is 16gb vRAM (5080) enough to train models like flux klein or ZiB?</a>
    </h3>

    
    <p class="card-summary">As the title says, I have trained a few ZiB models and Zit models on thing alike runpod + ostris, using the default settings and such and renting a 5090, and it goes very well, and fast (which I assume is due to the GDDR7), and im looking to upgrade my GPU. Would a 5080 be able to do similar? On the rented 5090, I&#39;m often at 14-16gb vRAM, so I wa shopping that once I upgrade I could instead try...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/MachineLearning</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">26m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/MachineLearning/comments/1r2muot/is_machine_learning_still_worth_it_in_2026_d/" target="_blank" rel="noopener">Is Machine Learning Still Worth It in 2026? [D]</a>
    </h3>

    
    <p class="card-summary">Is Machine Learning Still Worth It in 2026? I’ve been learning machine learning for a while now and I haave got a solid understanding of the basics training models, tuning them, avoiding overfitting and all that and lately some people around me keep asking if it’s even worth getting into in 2026. I’m kind of past that is this a trend? AI is everywhere right now and every company claims they’re...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/StableDiffusion</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">27m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/StableDiffusion/comments/1r2mugs/question_about_zimage_skin_texture/" target="_blank" rel="noopener">Question about Z-Image skin texture</a>
    </h3>

    
    <p class="card-summary">Very stupid question! No matter what, I just cannot seem to get Z-Image to create realstic looking humans, and always end up with that creepy plastic doll skin! I&#39;ve followed a few tutorials with really simple Comfy workflows, so I&#39;m somewhat at my wits end here. Prompt adherence is fine, faces, limbs, backgrounds, mostly good enough. Skin... Looks like a perfect smooth plastic AI doll. What the...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/ChatGPT</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">36m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/ChatGPT/comments/1r2mooz/this_morning_chatgpt_talked_me_out_of_toughing/" target="_blank" rel="noopener">This morning ChatGPT talked me out of toughing out a strain in my calf muscle and to go get it looked at because it suspected a blood clot.</a>
    </h3>

    
    <p class="card-summary">It was correct and I have a massive amount of clots that made their way into both lungs and I would have died if I waited one more day. Thanks ChatGPT for insisting I call in sick and head to the ER immediately &amp;#32; submitted by &amp;#32; /u/Substantial-Fall-630 [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/LocalLLaMA</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">36m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/LocalLLaMA/comments/1r2moge/lobotomyless_reap_by_samsung_ream/" target="_blank" rel="noopener">Lobotomy-less REAP by Samsung (REAM)</a>
    </h3>

    
    <p class="card-summary">Samsung recently have pushed an alternative way to shrink a model instead of the usual REAP done by Cerebras with Kimi-Linear / DeepSeek v3.2 / GLM 4.X / MiniMax M2* / Qwen3* ... But Samsung might be cooking something else that are less damaging with REAM. https://bknyaz.github.io/blog/2026/moe/ Qwen3-Coder-Next-REAM-60B (from the recent 80B-A3B update)...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/LocalLLaMA</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">42m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/LocalLLaMA/comments/1r2mkz7/minimax_m25_weights_to_drop_soon/" target="_blank" rel="noopener">Minimax M2.5 weights to drop soon</a>
    </h3>

    
    <p class="card-summary">At least there’s official confirmation now. &amp;#32; submitted by &amp;#32; /u/No_Conversation9561 [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/SideProject</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">43m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/SideProject/comments/1r2mkdq/i_turned_claude_code_into_a_job_hunting_toolkit/" target="_blank" rel="noopener">I turned Claude Code into a job hunting toolkit: CV tailoring, mock interviews, no code, just markdown</a>
    </h3>

    
    <p class="card-summary">Paste a job ad, get a CV tailored to that specific role. Then practice the interview until you stop screwing up. I got tired of rewriting my CV from scratch for every freelance application, so I built a framework that runs entirely inside Claude Code. No code, no API keys, just structured markdown and slash commands. What it does: - Import your existing CV (or multiple docs, LinkedIn exports...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/startups</span>
        <span class="card-category" style="background: #EC4899;">
            Deals &amp; Acquisitions
        </span>
        <span class="card-time">46m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/startups/comments/1r2miwc/10_months_in_still_at_123_mrri_knew_journey_would/" target="_blank" rel="noopener">10 months in, still at $123 mrr.I knew Journey Would Be Gruelling, but it gets worse with each failed app. I will not promote</a>
    </h3>

    
    <p class="card-summary">launched a solo iOS app studio 10 months ago. Since then I’ve shipped 8 apps and made $1,480 total, which works out to roughly $123/month on average. Revenue has been 100% organic discovery so far. I spent $580 on ads and got basically nothing meaningful from it. The most frustrating part: the “quick” apps I built in 3–7 days (almost throwaway experiments) generated about 80% of the revenue. The...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/ChatGPT</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">50m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/ChatGPT/comments/1r2mg8j/asked_chatgpt_how_cleopatra_may_have_looked_like/" target="_blank" rel="noopener">Asked ChatGPT how cleopatra may have looked like……..</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/HierAdil [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">CoinDesk</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">56m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.coindesk.com/markets/2026/02/12/trump-linked-wlfi-s-zak-folkman-teases-forex-platform-at-consensus-hong-kong" target="_blank" rel="noopener">Trump-linked WLFI&#39;s Zak Folkman teases forex platform at Consensus Hong Kong</a>
    </h3>

    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/LocalLLaMA</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">57m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/LocalLLaMA/comments/1r2mbyp/thank_you_chinese_devs_for_providing_for_the/" target="_blank" rel="noopener">Thank you Chinese devs for providing for the community if it not for them we&#39;ll be still stuck 2020</a>
    </h3>

    
    <p class="card-summary">&amp;#32; submitted by &amp;#32; /u/dead-supernova [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/startups</span>
        <span class="card-category" style="background: #EC4899;">
            Deals &amp; Acquisitions
        </span>
        <span class="card-time">58m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/startups/comments/1r2mbes/my_cofounder_suddenly_wants_a_5050_split_i_will/" target="_blank" rel="noopener">my cofounder suddenly wants a 50-50 split i will not promote</a>
    </h3>

    
    <p class="card-summary">our split has been 60/40. we&#39;ve incorporated with this structure also. a vc who&#39;ll potentially lead our seed round is meeting us a few days from now for their verdict (and we&#39;re feeling good about this since we&#39;ve cleared dd) and my co-founder suddenly wants a 50/50 split. so a little bit about me: i founded our startup, later hired her as an intern initially, then promoted her to cofounder role...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/SideProject</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">59m ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/SideProject/comments/1r2mbai/i_built_4_openclaws_in_4_hours_heres_the/" target="_blank" rel="noopener">I built 4 OpenClaws in 4 hours - here&#39;s the architecture and results</a>
    </h3>

    
    <p class="card-summary">TL;DR: Spent 4 hours building a 3-agent orchestration system using OpenClaw (open-source Claude orchestration framework). After 3 days of running: went from 20 to 100 Reddit karma, got 6 meaningful comments, 9 Twitter replies, and 100% automation success rate. No code required, just markdown config files. This post breaks down the architecture, shows real config examples, and shares what I...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/LocalLLaMA</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/LocalLLaMA/comments/1r2m5h7/cacheaware_prefilldecode_disaggregation_40_faster/" target="_blank" rel="noopener">Cache-aware prefill–decode disaggregation = 40% faster long-context LLM serving</a>
    </h3>

    
    <p class="card-summary">cache aware prefill-decode disagg for 40% faster long-context LLM serving even with vanilla PD disagg, long cold prompts block fast warm ones. here they split the cold new long prompt prefill workloads from the warm prefills Result: &amp;gt; ~40% higher QPS &amp;gt; lower, stabler TTFT &amp;gt; seconds → ms via KV reuse &amp;#32; submitted by &amp;#32; /u/incarnadine72 [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/defi</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/defi/comments/1r2m1oz/bear_market_perspective/" target="_blank" rel="noopener">Bear market perspective</a>
    </h3>

    
    <p class="card-summary">Genuine question for builders here: What are you working on right now? I ask because I&#39;m seeing a weird split: Retail: Full panic mode, charts red, portfolios down 50%+ Builders: Heads down shipping Some data points making me bullish on people who stayed: RWA TVL: $16.6B (14% of total DeFi) Institutional allocation growth: 62% held/increased positions Long-term belief: 94% of institutions still...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinTelegraph</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://cointelegraph.com/news/strategy-ceo-preferred-stock-fund-bitcoin-buys?utm_source=rss_feed&amp;utm_medium=rss&amp;utm_campaign=rss_partner_inbound" target="_blank" rel="noopener">Strategy CEO eyes more preferred stock to fund Bitcoin buys</a>
    </h3>

    
    <p class="card-summary">Strategy CEO Phong Le says the company is moving away from issuing and selling common stock to buy Bitcoin and isn&#39;t interested in acquiring Bitcoin treasuries.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/ethereum</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/ethereum/comments/1r2lnxp/daily_general_discussion_february_12_2026/" target="_blank" rel="noopener">Daily General Discussion February 12, 2026</a>
    </h3>

    
    <p class="card-summary">Welcome to the Daily General Discussion on r/ethereum https://imgur.com/3y7vezP Bookmarking this link will always bring you to the current daily: https://old.reddit.com/r/ethereum/about/sticky/?num=2 Please use this thread to discuss Ethereum topics, news, events, and even price! Price discussion posted elsewhere in the subreddit will continue to be removed. As always, be constructive. -...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/OpenAI</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/OpenAI/comments/1r2llm1/the_openclaw_security_situation_is_worse_than/" target="_blank" rel="noopener">the OpenClaw security situation is worse than most people realize — here&#39;s what I found going through every audit</a>
    </h3>

    
    <p class="card-summary">I&#39;ve been using OpenClaw for a while now and started digging into the security side because I wanted to connect it to my email. glad I did the research first. snyk scanned about 4,000 skills on ClawHub. 36% had vulnerabilities. 76 were actual malware. hacker news community did their own audit — 12% malicious. 1Password found keyloggers in popular-looking skills. the one that got me was ClawHavoc...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/ChatGPT</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/ChatGPT/comments/1r2llcd/when_52_chat_asks_do_you_want_a_warmer_response/" target="_blank" rel="noopener">When 5.2 chat asks, &#34;Do you want a warmer response?&#34;</a>
    </h3>

    
    <p class="card-summary">I just saw it pop up. Has anyone tried it yet? Hopefully it&#39;ll be good for those who miss 4o. &amp;#32; submitted by &amp;#32; /u/ponlapoj [link] &amp;#32; [comments]</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinTelegraph</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://cointelegraph.com/news/ether-prices-v-shaped-recovery-says-tom-lee?utm_source=rss_feed&amp;utm_medium=rss&amp;utm_campaign=rss_partner_inbound" target="_blank" rel="noopener">Ether set for another ‘V-shaped recovery,’ Tom Lee says</a>
    </h3>

    
    <p class="card-summary">Fundstrat&#39;s Tom Lee argues Ether is close to the bottom and says investors should be thinking about opportunities instead of selling.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/StableDiffusion</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/StableDiffusion/comments/1r2lkc2/acestep_15_amd_gpu_how_do_i_get_flash_attention/" target="_blank" rel="noopener">Ace-Step 1.5: AMD GPU + How do I get Flash Attention feature + limited audio duration and batch size</a>
    </h3>

    
    <p class="card-summary">I am running an AMD 7900 GRE GPU with 16 GB of VRAM. The installation went smoothly, and I have downloaded all the available models. However, not sure what I did wrong, but I am experiencing some limitations, listed below: I am unable to use the “Use Flash Attention” feature. Can someone guide me on how to install the necessary components to enable this? The audio duration is limited to only...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/LocalLLaMA</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/LocalLLaMA/comments/1r2lheu/running_mistral7b_on_intel_npu_126_tokenss_zero/" target="_blank" rel="noopener">Running Mistral-7B on Intel NPU — 12.6 tokens/s, zero CPU/GPU usage</a>
    </h3>

    
    <p class="card-summary">Got tired of my Intel NPU sitting there doing nothing, so I made a simple tool to run LLMs on it. Benchmarks (Core Ultra, Mistral-7B-int4): Device Decode Speed TTFT Memory NPU 12.63 t/s 1.8s 4.8 GB CPU 9.04 t/s 1.1s 7.3 GB iGPU 23.38 t/s 0.25s 4.1 GB Yes, iGPU is faster. But the point of NPU is that it&#39;s a dedicated accelerator — your CPU and GPU stay completely free while the model runs. I can...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/ethereum</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/ethereum/comments/1r2lh6f/everyone_talks_about_eth_l2s_what_about_bitcoin/" target="_blank" rel="noopener">everyone talks about eth l2s. what about bitcoin l2s? is stx actually interesting?</a>
    </h3>

    
    <p class="card-summary">bitcoin has a $1T+ market cap… and basically no native defi. stacks (stx) is trying to change that, smart contracts anchored to bitcoin, btc-secured, all that. the thesis sounds simple: if even a small % of btc liquidity flows into btc-native defi, that’s huge but i keep wondering: does bitcoin culture even want defi? is stx really a btc l2, or just another chain marketing itself that way? is...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinDesk</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">1h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.coindesk.com/business/2026/02/12/uk-appoints-hsbc-for-blockchain-bond-pilot" target="_blank" rel="noopener">UK appoints HSBC for blockchain bond pilot</a>
    </h3>

    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/MachineLearning</span>
        <span class="card-category" style="background: #06B6D4;">
            Social Buzz
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/MachineLearning/comments/1r2l6w4/d_is_a_kdd_publication_considered_prestigious_for/" target="_blank" rel="noopener">[D] Is a KDD publication considered prestigious for more theoretical results?</a>
    </h3>

    
    <p class="card-summary">I do work at the intersection of ML and exact sciences and have some quite technical results that I submitted to KDD because they had a very fitting new AI for science track and all other deadlines were far away. Slightly hesitating now if I made the right choice because scrolling through their previous papers it all seems more industry focused. People around me also all heard of neurips etc but...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">CoinTelegraph</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://cointelegraph.com/news/whatsapp-accuses-russia-of-trying-to-block-it-to-push-state-backed-app?utm_source=rss_feed&amp;utm_medium=rss&amp;utm_campaign=rss_partner_inbound" target="_blank" rel="noopener">Russia is blocking WhatsApp to push ‘surveillance’ app, company says</a>
    </h3>

    
    <p class="card-summary">Russian news outlets reported on Wednesday that WhatsApp’s domain had been completely blocked and was inaccessible without a VPN or another workaround.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">Decrypt</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://decrypt.co/357820/us-banks-urge-regulator-slow-crypto-linked-bank-charters" target="_blank" rel="noopener">US Banks Urge Regulator to Slow Crypto-Linked Charters Amid Rule Overhaul</a>
    </h3>

    
    <p class="card-summary">The warning comes as crypto firms push for deeper access to the U.S. banking system, heightening tensions over unresolved regulatory gaps.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">Hacker News - Front Page</span>
        <span class="card-category" style="background: #F59E0B;">
            New Tools &amp; Apps
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://github.com/tonyyont/peon-ping" target="_blank" rel="noopener">Warcraft III Peon Voice Notifications for Claude Code</a>
    </h3>

    
    <p class="card-summary">Article URL: https://github.com/tonyyont/peon-ping Comments URL: https://news.ycombinator.com/item?id=46985151 Points: 141 # Comments: 46</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">r/startups</span>
        <span class="card-category" style="background: #EC4899;">
            Deals &amp; Acquisitions
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://www.reddit.com/r/startups/comments/1r2kuh3/what_would_you_recommend_students_i_will_not/" target="_blank" rel="noopener">What Would You Recommend Students? - I will not promote</a>
    </h3>

    
    <p class="card-summary">I&#39;m a student founder at Duke University, and I want to make sure that I get the most out of my experience here. For people who&#39;ve graduated, looking back what university-related resources would you recommend students take advantage of? I&#39;m already fairly social and I understand that networking with peers and professors is the best advice, but I&#39;m looking for specific things at your university...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">Decrypt</span>
        <span class="card-category" style="background: #8B5CF6;">
            Web3 &amp; DeFi
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://decrypt.co/357818/bitcoin-crypto-this-weeks-inflation-data" target="_blank" rel="noopener">What to Expect for Bitcoin and Crypto Ahead of This Week&#39;s Inflation Data</a>
    </h3>

    
    <p class="card-summary">Investors are focused on January’s CPI release after stronger-than-expected jobs data forced a repricing of rate expectations.</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10118" target="_blank" rel="noopener">Reviewing the Reviewer: Elevating Peer Review Quality through LLM-Guided Feedback</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10118v1 Announce Type: new Abstract: Peer review is central to scientific quality, yet reliance on simple heuristics -- lazy thinking -- has lowered standards. Prior work treats lazy thinking detection as a single-label task, but review segments may exhibit multiple issues, including...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10229" target="_blank" rel="noopener">Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10229v1 Announce Type: new Abstract: While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10238" target="_blank" rel="noopener">Learning to Evict from Key-Value Cache</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10238v1 Announce Type: new Abstract: The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10298" target="_blank" rel="noopener">On Emergent Social World Models -- Evidence for Functional Integration of Theory of Mind and Pragmatic Reasoning in Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10298v1 Announce Type: new Abstract: This paper investigates whether LMs recruit shared computational mechanisms for general Theory of Mind (ToM) and language-specific pragmatic reasoning in order to contribute to the general question of whether LMs may be said to have emergent &#34;social...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10329" target="_blank" rel="noopener">Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10329v1 Announce Type: new Abstract: Human reasoning is shaped by resource rationality -- optimizing performance under constraints. Recently, inference-time scaling has emerged as a powerful paradigm to improve the reasoning performance of Large Language Models by expanding test-time...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10339" target="_blank" rel="noopener">The Subjectivity of Respect in Police Traffic Stops: Modeling Community Perspectives in Body-Worn Camera Footage</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10339v1 Announce Type: new Abstract: Traffic stops are among the most frequent police-civilian interactions, and body-worn cameras (BWCs) provide a unique record of how these encounters unfold. Respect is a central dimension of these interactions, shaping public trust and perceived...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10346" target="_blank" rel="noopener">Geometry-Aware Decoding with Wasserstein-Regularized Truncation and Mass Penalties for Large Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10346v1 Announce Type: new Abstract: Large language models (LLMs) must balance diversity and creativity against logical coherence in open-ended generation. Existing truncation-based samplers are effective but largely heuristic, relying mainly on probability mass and entropy while...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10350" target="_blank" rel="noopener">When Less Is More? Diagnosing ASR Predictions in Sardinian via Layer-Wise Decoding</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10350v1 Announce Type: new Abstract: Recent studies have shown that intermediate layers in multilingual speech models often encode more phonetically accurate representations than the final output layer. In this work, we apply a layer-wise decoding strategy to a pretrained Wav2Vec2 model...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10352" target="_blank" rel="noopener">Learning Self-Interpretation from Interpretability Artifacts: Training Lightweight Adapters on Vector-Label Pairs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10352v1 Announce Type: new Abstract: Self-interpretation methods prompt language models to describe their own internal states, but remain unreliable due to hyperparameter sensitivity. We show that training lightweight adapters on interpretability artifacts, while keeping the LM entirely...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10354" target="_blank" rel="noopener">Physically Interpretable AlphaEarth Foundation Model Embeddings Enable LLM-Based Land Surface Intelligence</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10354v1 Announce Type: new Abstract: Satellite foundation models produce dense embeddings whose physical interpretability remains poorly understood, limiting their integration into environmental decision systems. Using 12.1 million samples across the Continental United States...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10356" target="_blank" rel="noopener">Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10356v1 Announce Type: new Abstract: Real-world digital environments are highly diverse and dynamic. These characteristics cause agents to frequently encounter unseen scenarios and distribution shifts, making continual learning in specific environments essential for computer-use agents...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10380" target="_blank" rel="noopener">The Alignment Bottleneck in Decomposition-Based Claim Verification</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10380v1 Announce Type: new Abstract: Structured claim decomposition is often proposed as a solution for verifying complex, multi-faceted claims, yet empirical results have been inconsistent. We argue that these inconsistencies stem from two overlooked bottlenecks: evidence alignment and...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10382" target="_blank" rel="noopener">Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10382v1 Announce Type: new Abstract: Backdoor attacks pose significant security risks for Large Language Models (LLMs), yet the internal mechanisms by which triggers operate remain poorly understood. We present the first mechanistic analysis of language-switching backdoors, studying the...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10384" target="_blank" rel="noopener">When Tables Go Crazy: Evaluating Multimodal Models on French Financial Documents</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10384v1 Announce Type: new Abstract: Vision-language models (VLMs) perform well on many document understanding tasks, yet their reliability in specialized, non-English domains remains underexplored. This gap is especially critical in finance, where documents mix dense regulatory text...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10388" target="_blank" rel="noopener">Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10388v1 Announce Type: new Abstract: The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10400" target="_blank" rel="noopener">When are We Worried? Temporal Trends of Anxiety and What They Reveal about Us</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10400v1 Announce Type: new Abstract: In this short paper, we make use of a recently created lexicon of word-anxiety associations to analyze large amounts of US and Canadian social media data (tweets) to explore *when* we are anxious and what insights that reveals about us. We show that...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10414" target="_blank" rel="noopener">EVOKE: Emotion Vocabulary Of Korean and English</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10414v1 Announce Type: new Abstract: This paper introduces EVOKE, a parallel dataset of emotion vocabulary in English and Korean. The dataset offers comprehensive coverage of emotion words in each language, in addition to many-to-many translations between words in the two languages and...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10454" target="_blank" rel="noopener">LATA: A Tool for LLM-Assisted Translation Annotation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10454v1 Announce Type: new Abstract: The construction of high-quality parallel corpora for translation research has increasingly evolved from simple sentence alignment to complex, multi-layered annotation tasks. This methodological shift presents significant challenges for structurally...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10480" target="_blank" rel="noopener">Neuro-Symbolic Synergy for Interactive World Modeling</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10480v1 Announce Type: new Abstract: Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10494" target="_blank" rel="noopener">Canvas-of-Thought: Grounding Reasoning via Mutable Structured States</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10494v1 Announce Type: new Abstract: While Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), relying solely on linear text sequences remains a bottleneck for complex tasks. We observe that even when...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10504" target="_blank" rel="noopener">On the Robustness of Knowledge Editing for Detoxification</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10504v1 Announce Type: new Abstract: Knowledge-Editing-based (KE-based) detoxification has emerged as a promising approach for mitigating harmful behaviours in Large Language Models. Existing evaluations, however, largely rely on automatic toxicity classifiers, implicitly assuming that...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10525" target="_blank" rel="noopener">LHAW: Controllable Underspecification for Long-Horizon Tasks</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10525v1 Announce Type: new Abstract: Long-horizon workflow agents that operate effectively over extended periods are essential for truly autonomous systems. Their reliable execution critically depends on the ability to reason through ambiguous situations in which clarification seeking is...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10560" target="_blank" rel="noopener">When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10560v1 Announce Type: new Abstract: While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10604" target="_blank" rel="noopener">Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10604v1 Announce Type: new Abstract: We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10609" target="_blank" rel="noopener">Online Causal Kalman Filtering for Stable and Effective Policy Optimization</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10609v1 Announce Type: new Abstract: Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10622" target="_blank" rel="noopener">How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10622v1 Announce Type: new Abstract: Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10652" target="_blank" rel="noopener">UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10652v1 Announce Type: new Abstract: Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10657" target="_blank" rel="noopener">Benchmarks Are Not That Out of Distribution: Word Overlap Predicts Performance</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10657v1 Announce Type: new Abstract: Understanding what constitutes high-quality pre-training data remains a central question in language model training. In this work, we investigate whether benchmark performance is primarily driven by the degree of statistical pattern overlap between...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10661" target="_blank" rel="noopener">Targeted Syntactic Evaluation of Language Models on Georgian Case Alignment</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10661v1 Announce Type: new Abstract: This paper evaluates the performance of transformer-based language models on split-ergative case alignment in Georgian, a particularly rare system for assigning grammatical cases to mark argument roles. We focus on subject and object marking...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10715" target="_blank" rel="noopener">Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10715v1 Announce Type: new Abstract: Long-term conversational memory is a core capability for LLM-based dialogue systems, yet existing benchmarks and evaluation protocols primarily focus on surface-level factual recall. In realistic interactions, appropriate responses often depend on...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10732" target="_blank" rel="noopener">Macaron: Controlled, Human-Written Benchmark for Multilingual and Multicultural Reasoning via Template-Filling</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10732v1 Announce Type: new Abstract: Multilingual benchmarks rarely test reasoning over culturally grounded premises: translated datasets keep English-centric scenarios, while culture-first datasets often lack control over the reasoning required. We propose Macaron, a template-first...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10740" target="_blank" rel="noopener">Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10740v1 Announce Type: new Abstract: Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10801" target="_blank" rel="noopener">Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10801v1 Announce Type: new Abstract: Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications. The core cause of hallucination lies in LLMs&#39; lack of awareness regarding their...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10816" target="_blank" rel="noopener">Beyond Confidence: The Rhythms of Reasoning in Generative Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10816v1 Announce Type: new Abstract: Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability. Conventional metrics like accuracy and perplexity fail to assess local prediction robustness, as...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10832" target="_blank" rel="noopener">I can tell whether you are a Native Hawl\^eri Speaker! How ANN, CNN, and RNN perform in NLI-Native Language Identification</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10832v1 Announce Type: new Abstract: Native Language Identification (NLI) is a task in Natural Language Processing (NLP) that typically determines the native language of an author through their writing or a speaker through their speaking. It has various applications in different areas...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10874" target="_blank" rel="noopener">C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10874v1 Announce Type: new Abstract: Automatic prompt optimization is a promising direction to boost the performance of Large Language Models (LLMs). However, existing methods often suffer from noisy and conflicting update signals. In this research, we propose C-MOP (Cluster-based...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10881" target="_blank" rel="noopener">Diagnosing Structural Failures in LLM-Based Evidence Extraction for Meta-Analysis</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10881v1 Announce Type: new Abstract: Systematic reviews and meta-analyses rely on converting narrative articles into structured, numerically grounded study records. Despite rapid advances in large language models (LLMs), it remains unclear whether they can meet the structural...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10886" target="_blank" rel="noopener">The CLEF-2026 FinMMEval Lab: Multilingual and Multimodal Evaluation of Financial AI Systems</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10886v1 Announce Type: new Abstract: We present the setup and the tasks of the FinMMEval Lab at CLEF 2026, which introduces the first multilingual and multimodal evaluation framework for financial Large Language Models (LLMs). While recent advances in financial natural language...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10908" target="_blank" rel="noopener">SoftMatcha 2: A Fast and Soft Pattern Matcher for Trillion-Scale Corpora</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10908v1 Announce Type: new Abstract: We present an ultra-fast and flexible search algorithm that enables search over trillion-scale natural language corpora in under 0.3 seconds while handling semantic variations (substitution, insertion, and deletion). Our approach employs string...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10947" target="_blank" rel="noopener">Computational Phenomenology of Temporal Experience in Autism: Quantifying the Emotional and Narrative Characteristics of Lived Unpredictability</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10947v1 Announce Type: new Abstract: Disturbances in temporality, such as desynchronization with the social environment and its unpredictability, are considered core features of autism with a deep impact on relationships. However, limitations regarding research on this issue include: 1)...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10953" target="_blank" rel="noopener">Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10953v1 Announce Type: new Abstract: Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10993" target="_blank" rel="noopener">LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10993v1 Announce Type: new Abstract: Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11028" target="_blank" rel="noopener">Linguistic Indicators of Early Cognitive Decline in the DementiaBank Pitt Corpus: A Statistical and Machine Learning Study</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11028v1 Announce Type: new Abstract: Background: Subtle changes in spontaneous language production are among the earliest indicators of cognitive decline. Identifying linguistically interpretable markers of dementia can support transparent and clinically grounded screening approaches...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11044" target="_blank" rel="noopener">Language Model Inversion through End-to-End Differentiation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11044v1 Announce Type: new Abstract: Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs. That is, given a LM and a desirable target output sequence of tokens, determining what input prompts would yield the target output remains an open...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11047" target="_blank" rel="noopener">Embedding Inversion via Conditional Masked Diffusion Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11047v1 Announce Type: new Abstract: We frame embedding inversion as conditional masked diffusion, recovering all tokens in parallel through iterative denoising rather than sequential autoregressive generation. A masked diffusion language model is conditioned on the target embedding via...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11065" target="_blank" rel="noopener">Conversational Behavior Modeling Foundation Model With Multi-Level Perception</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11065v1 Announce Type: new Abstract: Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this perceptual pathway is key to building natural full-duplex interactive systems. We introduce a framework that models this process as...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11072" target="_blank" rel="noopener">Simultaneous Speech-to-Speech Translation Without Aligned Data</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11072v1 Announce Type: new Abstract: Simultaneous speech translation requires translating source speech into a target language in real-time while handling non-monotonic word dependencies. Traditional approaches rely on supervised training with word-level aligned data, which is difficult...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11081" target="_blank" rel="noopener">SteuerLLM: Local specialized large language model for German tax law analysis</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11081v1 Announce Type: new Abstract: Large language models (LLMs) demonstrate strong general reasoning and language understanding, yet their performance degrades in domains governed by strict formal rules, precise terminology, and legally binding structure. Tax law exemplifies these...</p>
    

    
</article>
    
        <article class="article-card trending">
    
    <span class="trending-badge">TRENDING</span>
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11089" target="_blank" rel="noopener">DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11089v1 Announce Type: new Abstract: In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \emph{data recipe}, which comprises a data processing pipeline to transform...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11091" target="_blank" rel="noopener">Can Large Language Models Make Everyone Happy?</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11091v1 Announce Type: new Abstract: Misalignment in Large Language Models (LLMs) refers to the failure to simultaneously satisfy safety, value, and cultural dimensions, leading to behaviors that diverge from human expectations in real-world settings where these dimensions must co-occur...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11096" target="_blank" rel="noopener">Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11096v1 Announce Type: new Abstract: Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11106" target="_blank" rel="noopener">TEGRA: Text Encoding With Graph and Retrieval Augmentation for Misinformation Detection</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11106v1 Announce Type: new Abstract: Misinformation detection is a critical task that can benefit significantly from the integration of external knowledge, much like manual fact-checking. In this work, we propose a novel method for representing textual documents that facilitates the...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11149" target="_blank" rel="noopener">Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11149v1 Announce Type: new Abstract: Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10119" target="_blank" rel="noopener">Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10119v1 Announce Type: cross Abstract: Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS)...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10134" target="_blank" rel="noopener">Reverse-Engineering Model Editing on Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10134v1 Announce Type: cross Abstract: Large language models (LLMs) are pretrained on corpora containing trillions of tokens and, therefore, inevitably memorize sensitive information. Locate-then-edit methods, as a mainstream paradigm of model editing, offer a promising solution by...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10138" target="_blank" rel="noopener">Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10138v1 Announce Type: cross Abstract: Chart understanding is a quintessential information fusion task, requiring the seamless integration of graphical and textual data to extract meaning. The advent of Multimodal Large Language Models (MLLMs) has revolutionized this domain, yet the...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10146" target="_blank" rel="noopener">VERA: Identifying and Leveraging Visual Evidence Retrieval Heads in Long-Context Understanding</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10146v1 Announce Type: cross Abstract: While Vision-Language Models (VLMs) have shown promise in textual understanding, they face significant challenges when handling long context and complex reasoning tasks. In this paper, we dissect the internal mechanisms governing long-context...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10161" target="_blank" rel="noopener">Omni-Safety under Cross-Modality Conflict: Vulnerabilities, Dynamics Mechanisms and Efficient Alignment</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10161v1 Announce Type: cross Abstract: Omni-modal Large Language Models (OLLMs) greatly expand LLMs&#39; multimodal capabilities but also introduce cross-modal safety risks. However, a systematic understanding of vulnerabilities in omni-modal interactions remains lacking. To bridge this gap...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10177" target="_blank" rel="noopener">Towards Autonomous Mathematics Research</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10177v1 Announce Type: cross Abstract: Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10231" target="_blank" rel="noopener">Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10231v1 Announce Type: cross Abstract: Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10271" target="_blank" rel="noopener">MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10271v1 Announce Type: cross Abstract: Understanding multimodal long-context documents that comprise multimodal chunks such as paragraphs, figures, and tables is challenging due to (1) cross-modal heterogeneity to localize relevant information across modalities, (2) cross-page reasoning...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10324" target="_blank" rel="noopener">Discovering Differences in Strategic Behavior Between Humans and LLMs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10324v1 Announce Type: cross Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10377" target="_blank" rel="noopener">Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10377v1 Announce Type: cross Abstract: Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10404" target="_blank" rel="noopener">Modular Multi-Task Learning for Chemical Reaction Prediction</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10404v1 Announce Type: cross Abstract: Adapting large language models (LLMs) trained on broad organic chemistry to smaller, domain-specific reaction datasets is a key challenge in chemical and pharmaceutical R&amp;amp;D. Effective specialisation requires learning new reaction knowledge while...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10408" target="_blank" rel="noopener">Gated Removal of Normalization in Transformers Enables Stable Training and Efficient Inference</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10408v1 Announce Type: cross Abstract: Normalization is widely viewed as essential for stabilizing Transformer training. We revisit this assumption for pre-norm Transformers and ask to what extent sample-dependent normalization is needed inside Transformer blocks. We introduce TaperNorm...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10416" target="_blank" rel="noopener">AI-rithmetic</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10416v1 Announce Type: cross Abstract: Modern AI systems have been successfully deployed to win medals at international math competitions, assist with research workflows, and prove novel technical lemmas. However, despite their progress at advanced levels of mathematics, they remain...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10437" target="_blank" rel="noopener">Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10437v1 Announce Type: cross Abstract: Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL)...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10453" target="_blank" rel="noopener">The Landscape of Prompt Injection Threats in LLM Agents: From Taxonomy to Analysis</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10453v1 Announce Type: cross Abstract: The evolution of Large Language Models (LLMs) has resulted in a paradigm shift towards autonomous agents, necessitating robust security against Prompt Injection (PI) vulnerabilities where untrusted inputs hijack agent behaviors. This SoK presents a...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10471" target="_blank" rel="noopener">TestExplora: Benchmarking LLMs for Proactive Bug Discovery via Repository-Level Test Generation</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10471v1 Announce Type: cross Abstract: Given that Large Language Models (LLMs) are increasingly applied to automate software development, comprehensive software assurance spans three distinct goals: regression prevention, reactive reproduction, and proactive discovery. Current...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10620" target="_blank" rel="noopener">ISD-Agent-Bench: A Comprehensive Benchmark for Evaluating LLM-based Instructional Design Agents</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10620v1 Announce Type: cross Abstract: Large Language Model (LLM) agents have shown promising potential in automating Instructional Systems Design (ISD), a systematic approach to developing educational programs. However, evaluating these agents remains challenging due to the lack of...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10625" target="_blank" rel="noopener">To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10625v1 Announce Type: cross Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10716" target="_blank" rel="noopener">RE-LLM: Refining Empathetic Speech-LLM Responses by Integrating Emotion Nuance</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10716v1 Announce Type: cross Abstract: With generative AI advancing, empathy in human-AI interaction is essential. While prior work focuses on emotional reflection, emotional exploration, key to deeper engagement, remains overlooked. Existing LLMs rely on text which captures limited...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10718" target="_blank" rel="noopener">SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10718v1 Announce Type: cross Abstract: While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10735" target="_blank" rel="noopener">Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10735v1 Announce Type: cross Abstract: A narrated e-book combines synchronized audio with digital text, highlighting the currently spoken word or sentence during playback. This format supports early literacy and assists individuals with reading challenges, while also allowing general...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10833" target="_blank" rel="noopener">Training-Induced Bias Toward LLM-Generated Content in Dense Retrieval</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10833v1 Announce Type: cross Abstract: Dense retrieval is a promising approach for acquiring relevant context or world knowledge in open-domain natural language processing tasks and is now widely used in information retrieval applications. However, recent reports claim a broad preference...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10959" target="_blank" rel="noopener">Rotary Positional Embeddings as Phase Modulation: Theoretical Bounds on the RoPE Base for Long-Context Transformers</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10959v1 Announce Type: cross Abstract: Rotary positional embeddings (RoPE) are widely used in large language models to encode token positions through multiplicative rotations, yet their behavior at long context lengths remains poorly characterized. In this work, we reinterpret RoPE as...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.10996" target="_blank" rel="noopener">The emergence of numerical representations in communicating artificial agents</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.10996v1 Announce Type: cross Abstract: Human languages provide efficient systems for expressing numerosities, but whether the sheer pressure to communicate is enough for numerical representations to arise in artificial agents, and whether the emergent codes resemble human numerals at...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11008" target="_blank" rel="noopener">ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11008v1 Announce Type: cross Abstract: We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11040" target="_blank" rel="noopener">Learning Page Order in Shuffled WOO Releases</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11040v1 Announce Type: cross Abstract: We investigate document page ordering on 5,461 shuffled WOO documents (Dutch freedom of information releases) using page embeddings. These documents are heterogeneous collections such as emails, legal texts, and spreadsheets compiled into single...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11052" target="_blank" rel="noopener">GraphSeek: Next-Generation Graph Analytics with LLMs</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11052v1 Announce Type: cross Abstract: Graphs are foundational across domains but remain hard to use without deep expertise. LLMs promise accessible natural language (NL) graph analytics, yet they fail to process industry-scale property graphs effectively and efficiently: such datasets...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11073" target="_blank" rel="noopener">Chatting with Images for Introspective Visual Thinking</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11073v1 Announce Type: cross Abstract: Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &#39;&#39;thinking with images&#39;&#39; attempts to...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11103" target="_blank" rel="noopener">GameDevBench: Evaluating Agentic Capabilities Through Game Development</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11103v1 Announce Type: cross Abstract: Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11133" target="_blank" rel="noopener">Just on Time: Token-Level Early Stopping for Diffusion Language Models</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11133v1 Announce Type: cross Abstract: Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11137" target="_blank" rel="noopener">Weight Decay Improves Language Model Plasticity</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11137v1 Announce Type: cross Abstract: The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily...</p>
    

    
</article>
    
        <article class="article-card">
    

    <div class="card-meta">
        <span class="card-source">ArXiv - Computation &amp; Language</span>
        <span class="card-category" style="background: #4F46E5;">
            AI &amp; LLMs
        </span>
        <span class="card-time">2h ago</span>
    </div>

    <h3 class="card-title">
        <a href="https://arxiv.org/abs/2602.11151" target="_blank" rel="noopener">Diffusion-Pretrained Dense and Contextual Embeddings</a>
    </h3>

    
    <p class="card-summary">arXiv:2602.11151v1 Announce Type: cross Abstract: In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through...</p>
    

    
</article>
    
</div>


    </main>

    <footer class="site-footer">
        <div class="container">
            <p>Last updated: February 12, 2026 at 07:37 UTC</p>
            <p>678 articles from 21 sources</p>
            <p>Daily Signal Feed &mdash; AI, Web3 &amp; Emerging Tech Aggregator</p>
        </div>
    </footer>

    <script>
    // Client-side search filter
    document.addEventListener('DOMContentLoaded', function() {
        const searchBar = document.querySelector('.search-bar');
        if (searchBar) {
            searchBar.addEventListener('input', function(e) {
                const query = e.target.value.toLowerCase();
                const cards = document.querySelectorAll('.article-card, .archive-item');
                cards.forEach(function(card) {
                    const text = card.textContent.toLowerCase();
                    card.style.display = text.includes(query) ? '' : 'none';
                });
            });
        }
    });
    </script>
</body>
</html>